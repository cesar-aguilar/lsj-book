[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "learning statistics with jamovi",
    "section": "",
    "text": "This textbook covers the contents of an introductory statistics class, as typically taught to undergraduate psychology, health or social science students. The book covers how to get started in jamovi as well as giving an introduction to data manipulation. From a statistical perspective, the book discusses descriptive statistics and graphing first, followed by chapters on probability theory, sampling and estimation, and null hypothesis testing. After introducing the theory, the book covers the analysis of contingency tables, correlation, t-tests, regression, ANOVA and factor analysis. Bayesian statistics are touched on at the end of the book.\nCitation: Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15"
  },
  {
    "objectID": "Preface.html",
    "href": "Preface.html",
    "title": "Preface",
    "section": "",
    "text": "This book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "Preface.html#preface-to-version-0.75",
    "href": "Preface.html#preface-to-version-0.75",
    "title": "Preface",
    "section": "Preface to Version 0.75",
    "text": "Preface to Version 0.75\nIn this version we have updated the figures, images and text to maintain compatibility with latest versions of jamovi (2.2); many thanks to Peter Fisk for his help with this. Also tweaked and corrected are a few sections where improvements have been suggested by readers. This has mainly included fixing typos but also in places correcting conceptual detail, for example we have updated the information on kurtosis to reflect that it isn’t really about distribution “pointiness” and instead kurtosis is about whether data distributions have thin or fat tails. Thanks to all the readers who made suggestions, either through contacting me by email, or raising an issue on github.\nDavid Foxcroft\nFebruary 9th, 2022"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.70",
    "href": "Preface.html#preface-to-version-0.70",
    "title": "Preface",
    "section": "Preface to Version 0.70",
    "text": "Preface to Version 0.70\nThis update from version 0.65 introduces some new analyses. In the ANOVA chapters we have added sections on repeated measures ANOVA and analysis of covariance (ANCOVA). In a new chapter we have introduced Factor Analysis and related techniques. Hopefully the style of this new material is consistent with the rest of the book, though eagle-eyed readers might spot a bit more of an emphasis on conceptual and practical explanations, and a bit less algebra. I’m not sure this is a good thing, and might add the algebra in a bit later. But it reflects both my approach to understanding and teaching statistics, and also some feedback I have received from students on a course I teach. In line with this, I have also been through the rest of the book and tried to separate out some of the algebra by putting it into a box or frame. It’s not that this stuff is not important or useful, but for some students they may wish to skip over it and therefore the boxing of these parts should help some readers.\nWith this version I am very grateful to comments and feedback received from my students and colleagues, notably Wakefield Morys-Carter, and also to numerous people all over the world who have sent in small suggestions and corrections - much appreciated, and keep them coming! One pretty neat new feature is that the example data files for the book can now be loaded into jamovi as an add-on module - thanks to Jonathon Love for helping with that.\nDavid Foxcroft\nFebruary 1st, 2019"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.65",
    "href": "Preface.html#preface-to-version-0.65",
    "title": "Preface",
    "section": "Preface to Version 0.65",
    "text": "Preface to Version 0.65\nIn this adaptation of the excellent ‘Learning statistics with R’, by Danielle Navarro, we have replaced the statistical software used for the analyses and examples with jamovi. Although R is a powerful statistical programming language, it is not the first choice for every instructor and student at the beginning of their statistical learning. Some instructors and students tend to prefer the point-and-click style of software, and that’s where jamovi comes in. jamovi is software that aims to simplify two aspects of using R. It offers a point-and-click graphical user interface (GUI), and it also provides functions that combine the capabilities of many others, bringing a more SPSS- or SAS-like method of programming to R. Importantly, jamovi will always be free and open - that’s one of its core values - because jamovi is made by the scientific community, for the scientific community.\nWith this version I am very grateful for the help of others who have read through drafts and provided excellent suggestions and corrections, particularly Dr David Emery and Kirsty Walter.\nDavid Foxcroft\nJuly 1st, 2018"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.6",
    "href": "Preface.html#preface-to-version-0.6",
    "title": "Preface",
    "section": "Preface to Version 0.6",
    "text": "Preface to Version 0.6\nThe book hasn’t changed much since 2015 when I released Version 0.5 – it’s probably fair to say that I’ve changed more than it has. I moved from Adelaide to Sydney in 2016 and my teaching profile at UNSW is different to what it was at Adelaide, and I haven’t really had a chance to work on it since arriving here! It’s a little strange looking back at this actually. A few quick comments…\n\nWeirdly, the book consistently misgenders me, but I suppose I have only myself to blame for that one :-) There’s now a brief footnote on page 12 that mentions this issue; in real life I’ve been working through a gender affirmation process for the last two years and mostly go by she/her pronouns. I am, however, just as lazy as I ever was so I haven’t bothered updating the text in the book.\nFor Version 0.6 I haven’t changed much I’ve made a few minor changes when people have pointed out typos or other errors. In particular it’s worth noting the issue associated with the etaSquared function in the lsr package (which isn’t really being maintained any more) in Section 14.4. The function works fine for the simple examples in the book, but there are definitely bugs in there that I haven’t found time to check! So please take care with that one.\nThe biggest change really is the licensing! I’ve released it under a Creative Commons licence (CC BY-SA 4.0, specifically), and placed all the source files to the associated GitHub repository, if anyone wants to adapt it.\n\nMaybe someone would like to write a version that makes use of the tidyverse… I hear that’s become rather important to R these days :-)\nBest,\nDanielle Navarro"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.5",
    "href": "Preface.html#preface-to-version-0.5",
    "title": "Preface",
    "section": "Preface to Version 0.5",
    "text": "Preface to Version 0.5\nAnother year, another update. This time around, the update has focused almost entirely on the theory sections of the book. Chapters 9, 10 and 11 have been rewritten, hopefully for the better. Along the same lines, Chapter 17 is entirely new, and focuses on Bayesian statistics. I think the changes have improved the book a great deal. I’ve always felt uncomfortable about the fact that all the inferential statistics in the book are presented from an orthodox perspective, even though I almost always present Bayesian data analyses in my own work. Now that I’ve managed to squeeze Bayesian methods into the book somewhere, I’m starting to feel better about the book as a whole. I wanted to get a few other things done in this update, but as usual I’m running into teaching deadlines, so the update has to go out the way it is!\nDanielle Navarro\nFebruary 16, 2015"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.4",
    "href": "Preface.html#preface-to-version-0.4",
    "title": "Preface",
    "section": "Preface to Version 0.4",
    "text": "Preface to Version 0.4\nA year has gone by since I wrote the last preface. The book has changed in a few important ways: Chapters 3 and 4 do a better job of documenting some of the time saving features of Rstudio, Chapters 12 and 13 now make use of new functions in the lsr package for running chi-square tests and t tests, and the discussion of correlations has been adapted to refer to the new functions in the lsr package. The soft copy of 0.4 now has better internal referencing (i.e., actual hyperlinks between sections), though that was introduced in 0.3.1. There’s a few tweaks here and there, and many typo corrections (thank you to everyone who pointed out typos!), but overall 0.4 isn’t massively different from 0.3.\nI wish I’d had more time over the last 12 months to add more content. The absence of any discussion of repeated measures ANOVA and mixed models more generally really does annoy me. My excuse for this lack of progress is that my second child was born at the start of 2013, and so I spent most of last year just trying to keep my head above water. As a consequence, unpaid side projects like this book got sidelined in favour of things that actually pay my salary! Things are a little calmer now, so with any luck version 0.5 will be a bigger step forward.\nOne thing that has surprised me is the number of downloads the book gets. I finally got some basic tracking information from the website a couple of months ago, and (after excluding obvious robots) the book has been averaging about 90 downloads per day. That’s encouraging: there’s at least a few people who find the book useful!\nDanielle Navarro\nFebruary 4, 2014"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.3",
    "href": "Preface.html#preface-to-version-0.3",
    "title": "Preface",
    "section": "Preface to Version 0.3",
    "text": "Preface to Version 0.3\nThere’s a part of me that really doesn’t want to publish this book. It’s not finished.\nAnd when I say that, I mean it. The referencing is spotty at best, the chapter summaries are just lists of section titles, there’s no index, there are no exercises for the reader, the organisation is suboptimal, and the coverage of topics is just not comprehensive enough for my liking. Additionally, there are sections with content that I’m not happy with, figures that really need to be redrawn, and I’ve had almost no time to hunt down inconsistencies, typos, or errors. In other words, this book is not finished. If I didn’t have a looming teaching deadline and a baby due in a few weeks, I really wouldn’t be making this available at all.\nWhat this means is that if you are an academic looking for teaching materials, a Ph.D. student looking to learn R, or just a member of the general public interested in statistics, I would advise you to be cautious. What you’re looking at is a first draft, and it may not serve your purposes. If we were living in the days when publishing was expensive and the internet wasn’t around, I would never consider releasing a book in this form. The thought of someone shelling out $80 for this (which is what a commercial publisher told me it would retail for when they offered to distribute it) makes me feel more than a little uncomfortable. However, it’s the 21st century, so I can post the pdf on my website for free, and I can distribute hard copies via a print-on-demand service for less than half what a textbook publisher would charge. And so my guilt is assuaged, and I’m willing to share! With that in mind, you can obtain free soft copies and cheap hard copies online, from the following webpages:\nSoft copy: www.compcogscisydney.com/learning-statistics-with-r.html\nHard copy: www.lulu.com/content/13570633\n[Ed: these links are defunct, try this instead: learningstatisticswithr.com]\nEven so, the warning still stands: what you are looking at is Version 0.3 of a work in progress. If and when it hits Version 1.0, I would be willing to stand behind the work and say, yes, this is a textbook that I would encourage other people to use. At that point, I’ll probably start shamelessly flogging the thing on the internet and generally acting like a tool. But until that day comes, I’d like it to be made clear that I’m really ambivalent about the work as it stands.\nAll of the above being said, there is one group of people that I can enthusiastically endorse this book to: the psychology students taking our undergraduate research methods classes (DRIP and DRIP:A) in 2013. For you, this book is ideal, because it was written to accompany your stats lectures. If a problem arises due to a shortcoming of these notes, I can and will adapt content on the fly to fix that problem. Effectively, you’ve got a textbook written specifically for your classes, distributed for free (electronic copy) or at near-cost prices (hard copy). Better yet, the notes have been tested: Version 0.1 of these notes was used in the 2011 class, Version 0.2 was used in the 2012 class, and now you’re looking at the new and improved Version 0.3. I’m not saying these notes are titanium plated awesomeness on a stick – though if you wanted to say so on the student evaluation forms, then you’re totally welcome to – because they’re not. But I am saying that they’ve been tried out in previous years and they seem to work okay. Besides, there’s a group of us around to troubleshoot if any problems come up, and you can guarantee that at least one of your lecturers has read the whole thing cover to cover!\nOkay, with all that out of the way, I should say something about what the book aims to be. At its core, it is an introductory statistics textbook pitched primarily at psychology students. As such, it covers the standard topics that you’d expect of such a book: study design, descriptive statistics, the theory of hypothesis testing, t-tests, χ 2 tests, ANOVA and regression. However, there are also several chapters devoted to the R statistical package, including a chapter on data manipulation and another one on scripts and programming. Moreover, when you look at the content presented in the book, you’ll notice a lot of topics that are traditionally swept under the carpet when teaching statistics to psychology students. The Bayesian/frequentist divide is openly disussed in the probability chapter, and the disagreement between Neyman and Fisher about hypothesis testing makes an appearance. The difference between probability and density is discussed. A detailed treatment of Type I, II and III sums of squares for unbalanced factorial ANOVA is provided. And if you have a look in the Epilogue, it should be clear that my intention is to add a lot more advanced content.\nMy reasons for pursuing this approach are pretty simple: the students can handle it, and they even seem to enjoy it. Over the last few years I’ve been pleasantly surprised at just how little difficulty I’ve had in getting undergraduate psych students to learn R. It’s certainly not easy for them, and I’ve found I need to be a little charitable in setting marking standards, but they do eventually get there. Similarly, they don’t seem to have a lot of problems tolerating ambiguity and complexity in presentation of statistical ideas, as long as they are assured that the assessment standards will be set in a fashion that is appropriate for them. So if the students can handle it, why not teach it? The potential gains are pretty enticing. If they learn R, the students get access to CRAN, which is perhaps the largest and most comprehensive library of statistical tools in existence. And if they learn about probability theory in detail, it’s easier for them to switch from orthodox null hypothesis testing to Bayesian methods if they want to. Better yet, they learn data analysis skills that they can take to an employer without being dependent on expensive and proprietary software.\nSadly, this book isn’t the silver bullet that makes all this possible. It’s a work in progress, and maybe when it is finished it will be a useful tool. One among many, I would think. There are a number of other books that try to provide a basic introduction to statistics using R, and I’m not arrogant enough to believe that mine is better. Still, I rather like the book, and maybe other people will find it useful, incomplete though it is.\nDanielle Navarro\nJanuary 13, 2013"
  },
  {
    "objectID": "Prelude-Part-IV.html",
    "href": "Prelude-Part-IV.html",
    "title": "Prelude",
    "section": "",
    "text": "Part IV of the book is by far the most theoretical, focusing as it does on the theory of statistical inference. Over the next three chapters my goal is to give you an introduction to probability theory, sampling and estimation in Chapter 2 and statistical hypothesis testing in Chapter 3. Before we get started though, I want to say something about the big picture. Statistical inference is primarily about learning from data. The goal is no longer merely to describe our data but to use the data to draw conclusions about the world. To motivate the discussion I want to spend a bit of time talking about a philosophical puzzle known as the riddle of induction, because it speaks to an issue that will pop up over and over again throughout the book: statistical inference relies on assumptions. This sounds like a bad thing. In everyday life people say things like “you should never make assumptions”, and psychology classes often talk about assumptions and biases as bad things that we should try to avoid. From bitter personal experience I have learned never to say such things around philosophers!"
  },
  {
    "objectID": "Prelude-Part-IV.html#on-the-limits-of-logical-reasoning",
    "href": "Prelude-Part-IV.html#on-the-limits-of-logical-reasoning",
    "title": "Prelude",
    "section": "On the limits of logical reasoning",
    "text": "On the limits of logical reasoning\n\nThe whole art of war consists in getting at what is on the other side of the hill, or, in other words, in learning what we do not know from what we do.\n- Arthur Wellesley, 1st Duke of Wellington\n\nI am told that quote above came about as a consequence of a carriage ride across the countryside.1 He and his companion, J. W. Croker, were playing a guessing game, each trying to predict what would be on the other side of each hill. In every case it turned out that Wellesley was right and Croker was wrong. Many years later when Wellesley was asked about the game he explained that “the whole art of war consists in getting at what is on the other side of the hill”. Indeed, war is not special in this respect. All of life is a guessing game of one form or another, and getting by on a day to day basis requires us to make good guesses. So let’s play a guessing game of our own.\nSuppose you and I are observing the Wellesley-Croker competition and after every three hills you and I have to predict who will win the next one, Wellesley or Croker. Let’s say that W refers to a Wellesley victory and C refers to a Croker victory. After three hills, our data set looks like this:\n\\(WWW\\)\nOur conversation goes like this:\n\nyou: Three in a row doesn’t mean much. I suppose Wellesley might be better at this than Croker, but it might just be luck. Still, I’m a bit of a gambler. I’ll bet on Wellesley.\n\n\nme: I agree that three in a row isn’t informative and I see no reason to prefer Wellesley’s guesses over Croker’s. I can’t justify betting at this stage. Sorry. No bet for me.\n\nYour gamble paid off: three more hills go by and Wellesley wins all three. Going into the next round of our game the score is 1-0 in favour of you and our data set looks like this: \\(WWW\\) \\(WWW\\) I’ve organised the data into blocks of three so that you can see which batch corresponds to the observations that we had available at each step in our little side game. After seeing this new batch, our conversation continues:\n\nyou: Six wins in a row for Duke Wellesley. This is starting to feel a bit suspicious. I’m still not certain, but I reckon that he’s going to win the next one too.\n\n\nme: I guess I don’t see that. Sure, I agree that Wellesley has won six in a row, but I don’t see any logical reason why that means he’ll win the seventh one. No bet. you: Do you really think so? Fair enough, but my bet worked out last time and I’m okay with my choice.\n\nFor a second time you were right, and for a second time I was wrong. Wellesley wins the next three hills, extending his winning record against Croker to 9-0. The data set available to us is now this: \\(WWW\\) \\(WWW\\) \\(WWW\\) And our conversation goes like this:\n\nyou: Okay, this is pretty obvious. Wellesley is way better at this game. We both agree he’s going to win the next hill, right?\n\n\nme: Is there really any logical evidence for that? Before we started this game, there were lots of possibilities for the first 10 outcomes, and I had no idea which one to expect. \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\) was one possibility, but so was \\(WCC\\) \\(CWC\\) \\(WWC\\) \\(C\\) and \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) or even \\(CCC\\) \\(CCC\\) \\(CCC\\) \\(C\\). Because I had no idea what would happen so I’d have said they were all equally likely. I assume you would have too, right? I mean, that’s what it means to say you have “no idea”, isn’t it?\n\n\nyou: I suppose so.\n\n\nme: Well then, the observations we’ve made logically rule out all possibilities except two: \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) or \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). Both of these are perfectly consistent with the evidence we’ve encountered so far, aren’t they?\n\n\nyou: Yes, of course they are. Where are you going with this? me: So what’s changed then? At the start of our game, you’d have agreed with me that these are equally plausible and none of the evidence that we’ve encountered has discriminated between these two possibilities. Therefore, both of these possibilities remain equally plausible and I see no logical reason to prefer one over the other. So yes, while I agree with you that Wellesley’s run of 9 wins in a row is remarkable, I can’t think of a good reason to think he’ll win the 10th hill. No bet.\n\n\nyou: I see your point, but I’m still willing to chance it. I’m betting on Wellesley.\n\nWellesley’s winning streak continues for the next three hills. The score in the Wellesley-Croker game is now 12-0, and the score in our game is now 3-0. As we approach the fourth round of our game, our data set is this: \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) and the conversation continues:\n\nyou: Oh yeah! Three more wins for Wellesley and another victory for me. Admit it, I was right about him! I guess we’re both betting on Wellesley this time around, right?\n\n\nme: I don’t know what to think. I feel like we’re in the same situation we were in last round, and nothing much has changed. There are only two legitimate possibilities for a sequence of 13 hills that haven’t already been ruled out, \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) and \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). It’s just like I said last time. If all possible outcomes were equally sensible before the game started, shouldn’t these two be equally sensible now given that our observations don’t rule out either one? I agree that it feels like Wellesley is on an amazing winning streak, but where’s the logical evidence that the streak will continue?\n\n\nyou: I think you’re being unreasonable. Why not take a look at our scorecard, if you need evidence? You’re the expert on statistics and you’ve been using this fancy logical analysis, but the fact is you’re losing. I’m just relying on common sense and I’m winning. Maybe you should switch strategies.\n\n\nme: Hmm, that is a good point and I don’t want to lose the game, but I’m afraid I don’t see any logical evidence that your strategy is better than mine. It seems to me that if there were someone else watching our game, what they’d have observed is a run of three wins to you. Their data would look like this: \\(YYY\\). Logically, I don’t see that this is any different to our first round of watching Wellesley and Croker. Three wins to you doesn’t seem like a lot of evidence, and I see no reason to think that your strategy is working out any better than mine. If I didn’t think that \\(WWW\\) was good evidence then for Wellesley being better than Croker at their game, surely I have no reason now to think that YYY is good evidence that you’re better at ours?\n\n\nyou: Okay, now I think you’re being a jerk.\n\n\nme: I don’t see the logical evidence for that."
  },
  {
    "objectID": "Prelude-Part-IV.html#learning-without-making-assumptions-is-a-myth",
    "href": "Prelude-Part-IV.html#learning-without-making-assumptions-is-a-myth",
    "title": "Prelude",
    "section": "Learning without making assumptions is a myth",
    "text": "Learning without making assumptions is a myth\nThere are lots of different ways in which we could dissect this dialogue, but since this is a statistics book pitched at psychologists and not an introduction to the philosophy and psychology of reasoning, I’ll keep it brief. What I’ve described above is sometimes referred to as the riddle of induction. It seems entirely reasonable to think that a 12-0 winning record by Wellesley is pretty strong evidence that he will win the 13th game, but it is not easy to provide a proper logical justification for this belief. On the contrary, despite the obviousness of the answer, it’s not actually possible to justify betting on Wellesley without relying on some assumption that you don’t have any logical justification for.\nThe riddle of induction is most associated with the philosophical work of David Hume and more recently Nelson Goodman, but you can find examples of the problem popping up in fields as diverse as literature (Lewis Carroll) and machine learning (the “no free lunch” theorem). There really is something weird about trying to “learn what we do not know from what we do know”. The critical point is that assumptions and biases are unavoidable if you want to learn anything about the world. There is no escape from this, and it is just as true for statistical inference as it is for human reasoning. In the dialogue I was taking aim at your perfectly sensible inferences as a human being, but the common sense reasoning that you relied on is no different to what a statistician would have done. Your “common sense” half of the dialog relied on an implicit assumption that there exists some difference in skill between Wellesley and Croker, and what you were doing was trying to work out what that difference in skill level would be. My “logical analysis” rejects that assumption entirely. All I was willing to accept is that there are sequences of wins and losses and that I did not know which sequences would be observed. Throughout the dialogue I kept insisting that all logically possible data sets were equally plausible at the start of the Wellesely-Croker game, and the only way in which I ever revised my beliefs was to eliminate those possibilities that were factually inconsistent with the observations.\nThat sounds perfectly sensible on its own terms. In fact, it even sounds like the hallmark of good deductive reasoning. Like Sherlock Holmes, my approach was to rule out that which is impossible in the hope that what would be left is the truth. Yet as we saw, ruling out the impossible never led me to make a prediction. On its own terms everything I said in my half of the dialogue was entirely correct. An inability to make any predictions is the logical consequence of making “no assumptions”. In the end I lost our game because you did make some assumptions and those assumptions turned out to be right. Skill is a real thing, and because you believed in the existence of skill you were able to learn that Wellesley had more of it than Croker. Had you relied on a less sensible assumption to drive your learning you might not have won the game.\nUltimately there are two things you should take away from this. First, as I’ve said, you cannot avoid making assumptions if you want to learn anything from your data. But second, once you realise that assumptions are necessary it becomes important to make sure you make the right ones! A data analysis that relies on few assumptions is not necessarily better than one that makes many assumptions, it all depends on whether those assumptions are good ones for your data. As we go through the rest of this book I’ll often point out the assumptions that underpin a particular statistical technique, and how you can check whether those assumptions are sensible."
  },
  {
    "objectID": "07-Introduction-to-probability.html",
    "href": "07-Introduction-to-probability.html",
    "title": "1  Introduction to probability",
    "section": "",
    "text": "[God] has afforded us only the twilight … of Probability.\n– John Locke\nUp to this point in the book we’ve discussed some of the key ideas in experimental design, and we’ve talked a little about how you can summarise a data set. To a lot of people this is all there is to statistics: collecting all the numbers, calculating averages, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics and one of the least powerful. The bigger and more useful part of statistics is that it provides information that lets you make inferences about data.\nOnce you start thinking about statistics in these terms, that statistics is there to help us draw inferences from data, you start seeing examples of it everywhere. For instance, here’s a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):\nThis kind of remark is entirely unremarkable in the papers or in everyday life, but let’s have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I’m too lazy to track down the original survey so let’s just imagine that they called 1000 New South Wales (NSW) voters at random, and 230 (23%) of those claimed that they intended to vote for the Australian Labor Party (ALP). For the 2010 Federal election the Australian Electoral Commission reported 4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true ALP primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?\nThe answer to the question is pretty obvious. If I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the only 230 people out of the entire voting public who actually intend to vote ALP. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it’s a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.\nInferential statistics provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, the theory of statistical inference is built on top of probability theory. And it is to probability theory that we must now turn. This discussion of probability theory is basically background detail. There’s not a lot of statistics per se in this chapter, and you don’t need to understand this material in as much depth as the other chapters in this part of the book. Nevertheless, because probability theory does underpin so much of statistics, it’s worth covering some of the basics."
  },
  {
    "objectID": "07-Introduction-to-probability.html#how-are-probability-and-statistics-different",
    "href": "07-Introduction-to-probability.html#how-are-probability-and-statistics-different",
    "title": "1  Introduction to probability",
    "section": "1.1 How are probability and statistics different?",
    "text": "1.1 How are probability and statistics different?\nBefore we start talking about probability theory, it’s helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they’re not identical. Probability theory is “the doctrine of chances”. It’s a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:\n\nWhat are the chances of a fair coin coming up heads 10 times in a row?\nIf I roll a six sided dice twice, how likely is it that I’ll roll two sixes?\nHow likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?\nWhat are the chances that I’ll win the lottery?\n\nNotice that all of these questions have something in common. In each case the “truth of the world” is known and my question relates to the “what kind of events” will happen. In the first question I know that the coin is fair so there’s a 50% chance that any individual coin flip will come up heads. In the second question I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question I know that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known model of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example we can write down the model like this:\n\\[P(head)=0.5\\]\nwhich you can read as “the probability of heads is 0.5”. As we’ll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question I don’t actually know exactly what’s going to happen. Maybe I’ll get 10 heads, like the question says. But maybe I’ll get three heads. That’s the key thing. In probability theory the model is known but the data are not.\nSo that’s probability. What about statistics? Statistical questions work the other way around. In statistics we do not know the truth about the world. All we have is the data and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:\n\nIf my friend flips a coin 10 times and gets 10 heads are they playing a trick on me?\nIf five cards off the top of the deck are all hearts how likely is it that the deck was shuffled?\nIf the lottery commissioner’s spouse wins the lottery how likely is it that the lottery was rigged?\n\nThis time around the only thing we have are data. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to infer is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:\nH H H H H H H H H H H\nand what I’m trying to do is work out which “model of the world” I should put my trust in. If the coin is fair then the model I should adopt is one that says that the probability of heads is 0.5, that is P(heads) = 0.5. If the coin is not fair then I should conclude that the probability of heads is not 0.5, which we would write as \\(P(heads)\\ne{0.5}\\). In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works."
  },
  {
    "objectID": "07-Introduction-to-probability.html#what-does-probability-mean",
    "href": "07-Introduction-to-probability.html#what-does-probability-mean",
    "title": "1  Introduction to probability",
    "section": "1.2 What does probability mean?",
    "text": "1.2 What does probability mean?\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. But if you’ve ever had that experience in real life you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:\n\nThey’re robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However, they’re not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n1.2.1 The frequentist view\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the frequentist view and it defines probability as a long-run frequency. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first N flips, and calculate the proportion of heads \\(\\frac{N_H}{N}\\) every time. Table 1.1 shows what I’d get (I did literally flip coins to produce this!):\n\n\n\n\nTable 1.1:  Coin flips and proportion of heads \n\nnumber of flipsnumber of headsproportion\n\n100.00\n\n210.50\n\n320.67\n\n430.75\n\n540.80\n\n640.67\n\n740.57\n\n850.63\n\n960.67\n\n1070.70\n\n1180.73\n\n1280.67\n\n1390.69\n\n14100.71\n\n15100.67\n\n16100.63\n\n17100.59\n\n18100.56\n\n19100.53\n\n20110.55\n\n\n\n\n\nNotice that at the start of the sequence the proportion of heads fluctuates wildly, starting at \\(.00\\) and rising as high as \\(.80\\). Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of \\(.50\\). This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion \\(\\frac{N_H}{N}\\) as \\(N\\) increases. Actually, I did it four times just to make sure it wasn’t a fluke. The results are shown in Figure 1.1. As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.\nThe frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.1 Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. First, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only one 2 November 2048. There’s no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like “There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in Section 2.5).\n\n\n\n\n\nFigure 1.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you’ve seen eventually settles down and converges to the true probability of \\(0.5\\). Each panel shows four different simulated experiments. In each case we pretend we flipped a coin \\(1000\\) times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of \\(.5\\), if we’d extended the experiment for an infinite number of coin flips they would have\n\n\n\n\n\n\n1.2.2 The Bayesian view\nThe Bayesian view of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.\nHowever, in order for this approach to work we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win $5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it’s a bad bet to take. So we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n1.2.3 What’s the difference? And who is right?\nNow that you’ve seen each of these two views independently it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives you should have some sense of how to answer those questions.\nOkay, assuming you understand the difference then you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I’ll explain towards the end of the book. But I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” (Fisher, 1922, p. 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” (Meehl, 1967, p. 114). The history of statistics, as you might gather, is not devoid of entertainment.\nIn any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view, and I’ll revisit the topic in more depth in ?sec-Bayesian-statistics."
  },
  {
    "objectID": "07-Introduction-to-probability.html#basic-probability-theory",
    "href": "07-Introduction-to-probability.html#basic-probability-theory",
    "title": "1  Introduction to probability",
    "section": "1.3 Basic probability theory",
    "text": "1.3 Basic probability theory\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so I’m going to have to talk about my trousers.\n\n1.3.1 Introducing probability distributions\nOne of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really have, that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I’m so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my trousers in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\) it means the event \\(X\\) is impossible (i.e., I never wear those trousers). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if \\(P(X) = 0.5\\) it means that I wear those trousers half of the time.\nAt this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a probability distribution. For example, Table 1.2 shows an example of a probability distribution.\n\n\n\n\nTable 1.2:  A probability distribution for trouser wearing \n\nWhich trousers?LabelProbability\n\nBlue jeans\\(X_1 \\)\\(P(X_1)=.5 \\)\n\nGrey jeans\\(X_2 \\)\\(P(X_2)=.3 \\)\n\nBlack jeans\\(X_3 \\)\\(P(X_3)=.1 \\)\n\nBlack suit\\(X_4 \\)\\(P(X_4)=0 \\)\n\nBlue tracksuit\\(X_5 \\)\\(P(X_5)=.1 \\)\n\n\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see ?sec-Bar-graphs) to visualise this distribution, as shown in Figure 1.2. And, at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dani wears jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X1, X2, X3)\\). If any of these elementary events occurs then \\(E\\) is also said to have occurred. Having decided to write down the definition of the E this way, it’s pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are \\(.5\\), \\(.3\\) and \\(.1\\), the probability that I wear jeans is equal to \\(.9\\). is: we just add everything up. In this particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list, in Table 1.3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book I won’t do so here.\n\n\n\n\n\nFigure 1.2: A visual depiction of the ‘trousers’ probability distribution. There are five ‘elementary events’, corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1\n\n\n\n\n\n\n\n\nTable 1.3:  Some rules that probabilities satisfy \n\nEnglishNotationFormula\n\nnot A\\(P (\\neg A) \\)\\(1-P(A) \\)\n\nA or B\\(P(A \\cup B) \\)\\(P(A) + P(B) - P(A \\cap B) \\)\n\nA and B\\(P(A \\cap B) \\)\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-The-binomial-distribution",
    "href": "07-Introduction-to-probability.html#sec-The-binomial-distribution",
    "title": "1  Introduction to probability",
    "section": "1.4 The binomial distribution",
    "text": "1.4 The binomial distribution\nAs you might imagine, probability distributions vary enormously and there’s an enormous range of distributions out there. However, they aren’t all equally important. In fact, the vast majority of the content in this book relies on one of five distributions: the binomial distribution, the normal distribution, the t distribution, the \\(\\chi^2\\) (“chi-square”) distribution and the F distribution. Given this, what I’ll do over the next few sections is provide a brief introduction to all five of these, paying special attention to the binomial and the normal. I’ll start with the binomial distribution since it’s the simplest of the five.\n\n1.4.1 Introducing the binomial\nThe theory of probability originated in the attempt to describe how games of chance work, so it seems fitting that our discussion of the binomial distribution should involve a discussion of rolling dice and flipping coins. Let’s imagine a simple “experiment”. In my hot little hand I’m holding 20 identical six-sided dice. On one face of each die there’s a picture of a skull, the other five faces are all blank. If I proceed to roll all 20 dice, what’s the probability that I’ll get exactly 4 skulls? Assuming that the dice are fair, we know that the chance of any one die coming up skulls is 1 in 6. To say this another way, the skull probability for a single die is approximately .167. This is enough information to answer our question, so let’s have a look at how it’s done.\nAs usual, we’ll want to introduce some names and some notation. We’ll let \\(N\\) denote the number of dice rolls in our experiment, which is often referred to as the size parameter of our binomial distribution. We’ll also use \\(\\theta\\) to refer to the the probability that a single die comes up skulls, a quantity that is usually called the success probability of the binomial.2 Finally, we’ll use \\(X\\) to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of \\(X\\) is due to chance we refer to it as a random variable. In any case, now that we have all this terminology and notation we can use it to state the problem a little more precisely. The quantity that we want to calculate is the probability that \\(X = 4\\) given that we know that \\(\\theta = .167\\) and \\(N = 20\\). The general “form” of the thing I’m interested in calculating could be written as\n\\[P(X|\\theta,N)\\]\nand we’re interested in the special case where \\(X = 4, \\theta = .167\\) and \\(N = 20\\).\n[Additional technical detail 3]\nYeah, yeah. I know what you’re thinking: notation, notation, notation. Really, who cares? Very few readers of this book are here for the notation, so I should probably move on and talk about how to use the binomial distribution. I’ve included the formula for the binomial distribution in Table 1.2, since some readers may want to play with it themselves, but since most people probably don’t care that much and because we don’t need the formula in this book, I won’t talk about it in any detail. Instead, I just want to show you what the binomial distribution looks like.\nTo that end, Figure 1.3 plots the binomial probabilities for all possible values of \\(X\\) for our dice rolling experiment, from \\(X = 0\\) (no skulls) all the way up to \\(X = 20\\) (all skulls). Note that this is basically a bar chart, and is no different to the “trousers probability” plot I drew in Figure 1.2. On the horizontal axis we have all the possible events, and on the vertical axis we can read off the probability of each of those events. So, the probability of rolling \\(4\\) skulls out of \\(20\\) is about \\(0.20\\) (the actual answer is \\(0.2022036\\), as we’ll see in a moment). In other words, you’d expect that to happen about 20% of the times you repeated this experiment.\nTo give you a feel for how the binomial distribution changes when we alter the values of \\(theta\\) and \\(N\\), let’s suppose that instead of rolling dice I’m actually flipping coins. This time around, my experiment involves flipping a fair coin repeatedly and the outcome that I’m interested in is the number of heads that I observe. In this scenario, the success probability is now \\(\\theta = \\frac{1}{2}\\). Suppose I were to flip the coin \\(N = 20\\) times. In this example, I’ve changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as ?fig-fig7-4 shows, the main effect of this is to shift the whole distribution, as you’d expect. Okay, what if we flipped a coin \\(N = 100\\) times? Well, in that case we get ?fig-fig7-4 (b). The distribution stays roughly in the middle but there’s a bit more variability in the possible outcomes.\n\n\n\n\n\nFigure 1.3: The binomial distribution with size parameter of \\(N = 20\\) and an underlying success probability of \\(\\theta = \\frac{1}{6}\\). Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of X). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-The-normal-distribution",
    "href": "07-Introduction-to-probability.html#sec-The-normal-distribution",
    "title": "1  Introduction to probability",
    "section": "1.5 The normal distribution",
    "text": "1.5 The normal distribution\nWhile the binomial distribution is conceptually the simplest distribution to understand, it’s not the most important one. That particular honour goes to the normal distribution, also referred to as “the bell curve” or a “Gaussian distribution”. A normal distribution is described using two parameters: the mean of the distribution µ and the standard deviation of the distribution \\(\\sigma\\).\n\n\n\n\n\nFigure 1.4: Two binomial distributions, involving a scenario in which I’m flipping a fair coin, so the underlying success probability is \\(\\theta = \\frac{1}{2}\\). In panel (a), we assume I’m flipping the coin \\(N = 20\\) times. In panel (b) we assume that the coin is flipped \\(N = 100\\) times\n\n\n\n\n\n\n\nFigure 1.5: Two binomial distributions, involving a scenario in which I’m flipping a fair coin, so the underlying success probability is \\(\\theta = \\frac{1}{2}\\). In panel (a), we assume I’m flipping the coin \\(N = 20\\) times. In panel (b) we assume that the coin is flipped \\(N = 100\\) times\n\n\n\n\n\n\n\n\n\nFigure 1.6: The normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). The x-axis corresponds to the value of some variable, and the y-axis tells us something about how likely we are to observe that value. However, notice that the y-axis is labelled Probability Density and not Probability. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the \\(y\\) axis behave a bit oddly - the height of the curve here isn’t actually the probability of observing a particular x value. On the other hand, it is true that the heights of the curve tells you which \\(x\\) values are more likely (the higher ones!). (see Probability density section for all the annoying details)\n\n\n\n\n[Additional technical detail 4]\nLet’s try to get a sense for what it means for a variable to be normally distributed. To that end, have a look at Figure 1.6 which plots a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). You can see where the name “bell curve” comes from; it looks a bit like a bell. Notice that, unlike the plots that I drew to illustrate the binomial distribution, the picture of the normal distribution in Figure 1.6 shows a smooth curve instead of “histogram-like” bars. This isn’t an arbitrary choice, the normal distribution is continuous whereas the binomial is discrete. For instance, in the die rolling example from the last section it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls. The figures that I drew in the previous section reflected this fact. In Figure 1.3, for instance, there’s a bar located at \\(X = 3\\) and another one at \\(X = 4\\) but there’s nothing in between. Continuous quantities don’t have this constraint. For instance, suppose we’re talking about the weather. The temperature on a pleasant Spring day could be 23 degrees, 24 degrees, 23.9 degrees, or anything in between since temperature is a continuous variable. And so a normal distribution might be quite appropriate for describing Spring temperatures5\n\n\n\n\n\nFigure 1.7: An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of \\(\\mu = 4\\). The dashed line shows a normal distribution with a mean of \\(\\mu = 7\\). In both cases, the standard deviation is \\(\\sigma = 1\\). Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right\n\n\n\n\nWith this in mind, let’s see if we can’t get an intuition for how the normal distribution works. First, let’s have a look at what happens when we play around with the parameters of the distribution. To that end, Figure 1.7 plots normal distributions that have different means but have the same standard deviation. As you might expect, all of these distributions have the same “width”. The only difference between them is that they’ve been shifted to the left or to the right. In every other respect they’re identical. In contrast, if we increase the standard deviation while keeping the mean constant, the peak of the distribution stays in the same place but the distribution gets wider, as you can see in Figure 1.8. Notice, though, that when we widen the distribution the height of the peak shrinks. This has to happen, in the same way that the heights of the bars that we used to draw a discrete binomial distribution have to sum to 1, the total area under the curve for the normal distribution must equal 1. Before moving on, I want to point out one important characteristic of the normal distribution. Irrespective of what the actual mean and standard deviation are, \\(68.3\\%\\) of the area falls within 1 standard deviation of the mean. Similarly, \\(95.4\\%\\) of the distribution falls within 2 standard deviations of the mean, and \\((99.7\\%)\\) of the distribution is within 3 standard deviations. This idea is illustrated in Figure 1.9; see also Figure 1.10.\n\n\n\n\n\nFigure 1.8: An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of \\(\\mu = 5\\), but they have different standard deviations. The solid line plots a distribution with standard deviation \\(\\sigma = 1\\), and the dashed line shows a distribution with standard deviation \\(\\sigma = 2\\). As a consequence, both distributions are ‘centred’ on the same spot, but the dashed line is wider than the solid one\n\n\n\n\n\n\n\n\n\nFigure 1.9: The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). The shaded areas illustrate ‘areas under the curve’ for two important cases. In panel (a), we can see that there is a 68.3% chance that an observation will fall within one standard deviation of the mean. In panel (b), we see that there is a 95.4% chance that an observation will fall within two standard deviations of the mean\n\n\n\n\n\n\n\n\n\nFigure 1.10: Two more examples of the ‘area under the curve idea’. There is a 15.9% chance that an observation is one standard deviation below the mean or smaller (panel (a)), and a 34.1% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel (b)). Notice that if you add these two numbers together you get 15.9% + 34.1% = 50%. For normally distributed data, there is a 50% chance that an observation falls below the mean. And of course that also implies that there is a 50% chance that it falls above the mean\n\n\n\n\n```\n\n1.5.1 Probability density\nThere’s something I’ve been trying to hide throughout my discussion of the normal distribution, something that some introductory textbooks omit completely. They might be right to do so. This “thing” that I’m hiding is weird and counter-intuitive even by the admittedly distorted standards that apply in statistics. Fortunately, it’s not something that you need to understand at a deep level in order to do basic statistics. Rather, it’s something that starts to become important later on when you move beyond the basics. So, if it doesn’t make complete sense, don’t worry too much, but try to make sure that you follow the gist of it.\nThroughout my discussion of the normal distribution there’s been one or two things that don’t quite make sense. Perhaps you noticed that the y-axis in these figures is labelled “Probability Density” rather than density. Maybe you noticed that I used \\(P(X)\\) instead of \\(P(X)\\) when giving the formula for the normal distribution.\nAs it turns out, what is presented here isn’t actually a probability, it’s something else. To understand what that something is you have to spend a little time thinking about what it really means to say that \\(X\\) is a continuous variable. Let’s say we’re talking about the temperature outside. The thermometer tells me it’s \\(23\\) degrees, but I know that’s not really true. It’s not exactly \\(23\\) degrees. Maybe it’s \\(23.1\\) degrees, I think to myself. But I know that that’s not really true either because it might actually be \\(23.09\\) degrees. But I know that… well, you get the idea. The tricky thing with genuinely continuous quantities is that you never really know exactly what they are.\nNow think about what this implies when we talk about probabilities. Suppose that tomorrow’s maximum temperature is sampled from a normal distribution with mean \\(23\\) and standard deviation 1. What’s the probability that the temperature will be exactly \\(23\\) degrees? The answer is “zero”, or possibly “a number so close to zero that it might as well be zero”. Why is this? It’s like trying to throw a dart at an infinitely small dart board. No matter how good your aim, you’ll never hit it. In real life you’ll never get a value of exactly \\(23\\). It’ll always be something like \\(23.1\\) or \\(22.99998\\) or suchlike. In other words, it’s completely meaningless to talk about the probability that the temperature is exactly \\(23\\) degrees. However, in everyday language if I told you that it was \\(23\\) degrees outside and it turned out to be \\(22.9998\\) degrees you probably wouldn’t call me a liar. Because in everyday language “\\(23\\) degrees” usually means something like “somewhere between \\(22.5\\) and \\(23.5\\) degrees”. And while it doesn’t feel very meaningful to ask about the probability that the temperature is exactly \\(23\\) degrees, it does seem sensible to ask about the probability that the temperature lies between \\(22.5\\) and \\(23.5\\), or between \\(20\\) and \\(30\\), or any other range of temperatures.\nThe point of this discussion is to make clear that when we’re talking about continuous distributions it’s not meaningful to talk about the probability of a specific value. However, what we can talk about is the probability that the value lies within a particular range of values. To find out the probability associated with a particular range what you need to do is calculate the “area under the curve”. We’ve seen this concept already, in Figure 1.9 the shaded areas shown depict genuine probabilities (e.g., in Figure 1.9) it shows the probability of observing a value that falls within 1 standard deviation of the mean).\nOkay, so that explains part of the story. I’ve explained a little bit about how continuous probability distributions should be interpreted (i.e., area under the curve is the key thing). But what does the formula for ppxq that I described earlier actually mean? Obviously, \\(P(x)\\) doesn’t describe a probability, but what is it? The name for this quantity \\(P(x)\\) is a probability density, and in terms of the plots we’ve been drawing it corresponds to the height of the curve. The densities themselves aren’t meaningful in and of themselves, but they’re “rigged” to ensure that the area under the curve is always interpretable as genuine probabilities. To be honest, that’s about as much as you really need to know for now.6"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-Other-useful-distributions",
    "href": "07-Introduction-to-probability.html#sec-Other-useful-distributions",
    "title": "1  Introduction to probability",
    "section": "1.6 Other useful distributions",
    "text": "1.6 Other useful distributions\nThe normal distribution is the distribution that statistics makes most use of (for reasons to be discussed shortly), and the binomial distribution is a very useful one for lots of purposes. But the world of statistics is filled with probability distributions, some of which we’ll run into in passing. In particular, the three that will appear in this book are the t distribution, the \\(\\chi^2\\) distribution and the F distribution. I won’t give formulas for any of these, or talk about them in too much detail, but I will show you some pictures: Figure 1.11, Figure 1.12 and Figure 1.13.\n\n\n\n\n\nFigure 1.11: A \\(t\\) distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it’s not quite the same. For comparison purposes I’ve plotted a standard normal distribution as the dashed line\n\n\n\n\n\n\n\n\n\nFigure 1.12: \\(\\chi^2\\) distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution\n\n\n\n\n\n\n\n\n\nFigure 1.13: An \\(F\\) distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they’re not quite the same in general\n\n\n\n\n\nThe \\(t\\) distribution is a continuous distribution that looks very similar to a normal distribution, see Figure 1.11. Note that the “tails” of the t distribution are “heavier” (i.e., extend further outwards) than the tails of the normal distribution). That’s the important difference between the two. This distribution tends to arise in situations where you think that the data actually follow a normal distribution, but you don’t know the mean or standard deviation. We’ll run into this distribution again in ?sec-Comparing-two-means.\nThe \\(\\chi^2\\) distribution is another distribution that turns up in lots of different places. The situation in which we’ll see it is when doing categorical data analysis in ?sec-Categorical-data-analysis, but it’s one of those things that actually pops up all over the place. When you dig into the maths (and who doesn’t love doing that?), it turns out that the main reason why the \\(\\chi^2\\) distribution turns up all over the place is that if you have a bunch of variables that are normally distributed, square their values and then add them up (a procedure referred to as taking a “sum of squares”), this sum has a \\(\\chi^2\\) distribution. You’d be amazed how often this fact turns out to be useful. Anyway, here’s what a \\(\\chi^2\\) distribution looks like: Figure 1.12.\nThe \\(F\\) distribution looks a bit like a \\(\\chi^2\\) distribution, and it arises whenever you need to compare two \\(\\chi^2\\) distributions to one another. Admittedly, this doesn’t exactly sound like something that any sane person would want to do, but it turns out to be very important in real world data analysis. Remember when I said that \\(\\chi^2\\) turns out to be the key distribution when we’re taking a “sum of squares”? Well, what that means is if you want to compare two different “sums of squares”, you’re probably talking about something that has an F distribution. Of course, as yet I still haven’t given you an example of anything that involves a sum of squares, but I will in ?sec-Comparing-several-means-one-way-ANOVA. And that’s where we’ll run into the F distribution. Oh, and there’s a picture in Figure 1.13.\n\nOkay, time to wrap this section up. We’ve seen three new distributions: \\(\\chi^2\\)), \\(t\\) and \\(F\\). They’re all continuous distributions, and they’re all closely related to the normal distribution. The main thing for our purposes is that you grasp the basic idea that these distributions are all deeply related to one another, and to the normal distribution. Later on in this book we’re going to run into data that are normally distributed, or at least assumed to be normally distributed. What I want you to understand right now is that, if you make the assumption that your data are normally distributed, you shouldn’t be surprised to see \\(\\chi^2\\), \\(t\\) and \\(F\\) distributions popping up all over the place when you start trying to do your data analysis."
  },
  {
    "objectID": "07-Introduction-to-probability.html#summary",
    "href": "07-Introduction-to-probability.html#summary",
    "title": "1  Introduction to probability",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nIn this chapter we’ve talked about probability. We’ve talked about what probability means and why statisticians can’t agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution and spent a good chunk of the chapter talking about some of the more important probability distributions that statisticians work with. The section by section breakdown looks like this:\n\nProbability theory versus statistics: How are probability and statistics different?\nThe frequentist view versus The Bayesian view of probability\nBasic probability theory\nThe binomial distribution, The normal distribution, and Other useful distributions\n\nAs you’d expect, my coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the “simpler” task of documenting standard probability distributions is a big topic. I’ve described five standard probability distributions in this chapter, but sitting on my bookshelf I have a 45-chapter book called “Statistical Distributions” (Evans et al., 2011) that lists a lot more than that. Fortunately for you, very little of this is necessary. You’re unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won’t need them for this book, but it never hurts to know that there’s other possibilities out there.\nPicking up on that last point, there’s a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often “forget” to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it’s important to understand these things before moving onto the applications. For example, there are a lot of rules about what you’re “allowed” to say when doing statistical inference and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian vs. frequentist distinction. Similarly, in ?sec-Comparing-two-means we’re going to talk about something called the t-test, and if you really want to have a grasp of the mechanics of the t-test it really helps to have a sense of what a t-distribution actually looks like. You get the idea, I hope.\n\n\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical distributions (3rd ed). Wiley.\n\n\nFisher, R. A. (1922). On the mathematical foundation of theoretical statistics. Philosophical Transactions of the Royal Society A, 222, 309–368.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A methodological paradox. Philosophy of Science, 34, 103–115."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "",
    "text": "At the start of the last chapter I highlighted the critical distinction between descriptive statistics and inferential statistics. As discussed in ?sec-Descriptive-statistics, the role of descriptive statistics is to concisely summarise what we do know. In contrast, the purpose of inferential statistics is to “learn what we do not know from what we do”. Now that we have a foundation in probability theory we are in a good position to think about the problem of statistical inference. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two “big ideas”: estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but I’m going to witter on about sampling theory first because estimation theory doesn’t make sense until you understand sampling. As a consequence, this chapter divides naturally into two parts, the first three sections are focused on sampling theory, and the last two sections make use of sampling theory to discuss how statisticians think about estimation."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#samples-populations-and-sampling",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#samples-populations-and-sampling",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "2.1 Samples, populations and sampling",
    "text": "2.1 Samples, populations and sampling\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in ?sec-Descriptive-statistics this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n2.1.1 Defining a population\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n2.1.2 Simple random samples\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 2.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 2.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 2.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 2.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 2.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 2.3.\n\n\n\n\n\nFigure 2.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n2.1.3 Most samples are not simple random samples\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n2.1.4 How much does it matter if you don’t have a simple random sample?\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 2.1 and Figure 2.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n2.1.5 Population parameters and sample statistics\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (?sec-Introduction-to-psychological-measurement), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 1. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 2.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 2.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 2.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 2.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about Estimating population parameters using your sample statistics and also Estimating a confidence interval but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#the-law-of-large-numbers",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#the-law-of-large-numbers",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "2.2 The law of large numbers",
    "text": "2.2 The law of large numbers\nIn the previous section I showed you the results of one fictitious IQ experiment with a sample size of N = 100. The results were somewhat encouraging as the true population mean is 100 and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it? The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQs of 10,000 people. We can simulate the results of this experiment using jamovi. The IQsim.omv file is a jamovi data file. In this file I have generated 10,000 random numbers sampled from a normal distribution for a population with mean = 100 and sd = 15. This was done by computing a new variable using the = NORM(100,15) function. A histogram and density plot shows that this larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics. The mean IQ for the larger sample turns out to be 99.68 and the standard deviation is 14.90. These values are now very close to the true population. See Figure 2.5.\n\n\n\n\n\nFigure 2.5: A random sample drawn from a normal distribution using jamovi\n\n\n\n\nI feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it’s so bloody obvious that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob Bernoulli, one of the founders of probability theory, formalised this idea back in 1713 he was kind of a jerk about it. Here’s how he described the fact that we all share this intuition:\n\nFor even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one’s goal (Stigler, 1986, p. 65).\n\nOkay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct. It really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the law of large numbers. The law of large numbers is a mathematical law that applies to many different sample statistics but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that’s what the mean is… an average), so let’s look at that. When applied to the sample mean what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size “approaches” infinity (written as \\(N \\longrightarrow \\infty\\)), the sample mean approaches the population mean \\(\\bar{X} \\longrightarrow \\mu\\))3\nI don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sampling-distributions-and-the-central-limit-theorem",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sampling-distributions-and-the-central-limit-theorem",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "2.3 Sampling distributions and the central limit theorem",
    "text": "2.3 Sampling distributions and the central limit theorem\nThe law of large numbers is a very powerful tool but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a “long run guarantee”. In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life.\n\n[The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again. (Keynes, 1923, p. 80).\n\nAs in economics, so too in psychology and statistics. It is not enough to know that we will eventually arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my actual data set has a sample size of \\(N = 100\\). In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set!\n\n2.3.1 Sampling distribution of the mean\nWith this in mind, let’s abandon the idea that our studies will have sample sizes of 10,000 and consider instead a very modest experiment indeed. This time around we’ll sample \\(N = 5\\) people and measure their IQ scores. As before, I can simulate this experiment in jamovi = NORM(100,15) function, but I only need 5 participant IDs this time, not 10,000. These are the five numbers that jamovi generated:\n90 82 94 99 110\nThe mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less accurate than the previous experiment. Now imagine that I decided to replicate the experiment. That is, I repeat the procedure as closely as possible and I randomly sample 5 new people and measure their IQ. Again, jamovi allows me to simulate the results of this procedure, and generates these five numbers:\n78 88 111 111 117\nThis time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I obtain the results shown in Table 2.1, and as you can see the sample mean varies from one replication to the next.\n\n\n\n\nTable 2.1:  Ten replications of the IQ experiment, each with a sample size of ( N = 5 ) \n\nPerson 1Person 2Person 3Person 4Person 5Sample Mean\n\nRep. 19082949911095.0\n\nRep. 27888111111117101.0\n\nRep. 3111122919886101.6\n\nRep. 4989611999107103.8\n\nRep. 510511310310398104.4\n\nRep. 68189938511492.4\n\nRep. 71009310898133106.4\n\nRep. 810710010511785102.8\n\nRep. 98611910873116100.4\n\nRep. 109512611212076105.8\n\n\n\n\n\nNow suppose that I decided to keep going in this fashion, replicating this “five IQ scores” experiment over and over again. Every time I replicate the experiment I write down the sample mean. Over time, I’d be amassing a new data set, in which every experiment generates a single data point. The first 10 observations from my data set are the sample means listed in Table 2.1, so my data set starts out like this:\n95.0 101.0 101.6 103.8 104.4 …\nWhat if I continued like this for 10,000 replications, and then drew a histogram. Well that’s exactly what I did, and you can see the results in Figure 2.6. As this picture illustrates, the average of 5 IQ scores is usually between 90 and 110. But more importantly, what it highlights is that if we replicate an experiment over and over again, what we end up with is a distribution of sample means! (Table 2.1)) This distribution has a special name in statistics, it’s called the sampling distribution of the mean.\n\n\n\n\n\nFigure 2.6: The sampling distribution of the mean for the ‘five IQ scores experiment’. If you sample 5 people at random and calculate their average IQ you’ll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores\n\n\n\n\nSampling distributions are another important theoretical idea in statistics, and they’re crucial for understanding the behaviour of small samples. For instance, when I ran the very first “five IQ scores” experiment, the sample mean turned out to be 95. What the sampling distribution in Figure 2.6 tells us, though, is that the “five IQ scores” experiment is not very accurate. If I repeat the experiment, the sampling distribution tells me that I can expect to see a sample mean anywhere between 80 and 120.\n\n\n2.3.2 Sampling distributions exist for any sample statistic!\nOne thing to keep in mind when thinking about sampling distributions is that any sample statistic you might care to calculate has a sampling distribution. For example, suppose that each time I replicated the “five IQ scores” experiment I wrote down the largest IQ score in the experiment. This would give me a data set that started out like this:\n110 117 122 119 113 …\nDoing this over and over again would give me a very different sampling distribution, namely the sampling distribution of the maximum. The sampling distribution of the maximum of 5 IQ scores is shown in Figure 2.7. Not surprisingly, if you pick 5 people at random and then find the person with the highest IQ score, they’re going to have an above average IQ. Most of the time you’ll end up with someone whose IQ is measured in the 100 to 140 range.\n\n\n\n\n\nFigure 2.7: The sampling distribution of the maximum for the ‘five IQ scores experiment’. If you sample 5 people at random and select the one with the highest IQ score you’ll probably see someone with an IQ between 100 and 140\n\n\n\n\n\n\n\n\n\nFigure 2.8: An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel (a), each data set contained only a single observation, so the mean of each sample is just one person’s IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than any one person’s IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel (c)), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean\n\n\n\n\n\n\n2.3.3 The central limit theorem\nAt this point I hope you have a pretty good sense of what sampling distributions are, and in particular what the sampling distribution of the mean is. In this section I want to talk about how the sampling distribution of the mean changes as a function of sample size. Intuitively, you already know part of the answer. If you only have a few observations, the sample mean is likely to be quite inaccurate. If you replicate a small experiment and recalculate the mean you’ll get a very different answer. In other words, the sampling distribution is quite wide. If you replicate a large experiment and recalculate the sample mean you’ll probably get the same answer you got last time, so the sampling distribution will be very narrow. You can see this visually in Figure 2.8, showing that the bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the standard error. The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample mean, we often use the acronym SEM. As you can see just by looking at the picture, as the sample size \\(N\\) increases, the SEM decreases.\nOkay, so that’s one part of the story. However, there’s something I’ve been glossing over so far. All my examples up to this point have been based on the “IQ scores” experiments, and because IQ scores are roughly normally distributed I’ve assumed that the population distribution is normal. What if it isn’t normal? What happens to the sampling distribution of the mean? The remarkable thing is this, no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution. To give you a sense of this I ran some simulations. To do this, I started with the “ramped” distribution shown in the histogram in Figure 2.9. As you can see by comparing the triangular shaped histogram to the bell curve plotted by the black line, the population distribution doesn’t look very much like a normal distribution at all. Next, I simulated the results of a large number of experiments. In each experiment I took \\(N = 2\\) samples from this distribution, and then calculated the sample mean. Figure 2.9 (b) plots the histogram of these sample means (i.e., the sampling distribution of the mean for \\(N = 2\\)). This time, the histogram produces a \\(\\chi^2\\)-shaped distribution. It’s still not normal, but it’s a lot closer to the black line than the population distribution in Figure 2.9 (a). When I increase the sample size to \\(N = 4\\), the sampling distribution of the mean is very close to normal (Figure 2.9 (c)), and by the time we reach a sample size of N = 8 it’s almost perfectly normal. In other words, as long as your sample size isn’t tiny, the sampling distribution of the mean will be approximately normal no matter what your population distribution looks like!\n\n\n\n\n\nFigure 2.9: A demonstration of the central limit theorem. In panel (a), we have a non-normal population distribution, and panels (b)-(d) show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel (a). As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations\n\n\n\n\nOn the basis of these figures, it seems like we have evidence for all of the following claims about the sampling distribution of the mean.\n\nThe mean of the sampling distribution is the same as the mean of the population\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases\nThe shape of the sampling distribution becomes normal as the sample size increases\n\nAs it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the central limit theorem. Among other things, the central limit theorem tells us that if the population distribution has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the sampling distribution of the mean also has mean \\(\\mu\\) and the standard error of the mean is\n\\[SEM=\\frac{\\sigma}{\\sqrt{N}}\\]\nBecause we divide the population standard deviation \\(\\sigma\\) by the square root of the sample size N, the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.4\nThis result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much more reliable a large experiment is. It tells us why the normal distribution is, well, normal. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, “general” intelligence as measured by IQ is an average of a large number of “specific” skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#estimating-population-parameters",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#estimating-population-parameters",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "2.4 Estimating population parameters",
    "text": "2.4 Estimating population parameters\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.5 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n2.4.1 Estimating the population mean\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 2.2):\n\n\n\n\nTable 2.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n2.4.2 Estimating the population standard deviation\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 2.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 2.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n[1] 8.541514\n\n\n\n\n\nFigure 2.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce an estimated standard deviation of only 8.4, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 2.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.6 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 2.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on Estimating population parameters, the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in Estimating population parameters. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).7\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 2.3 and Table 2.4).\n\n\n\n\nTable 2.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 2.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "2.5 Estimating a confidence interval",
    "text": "2.5 Estimating a confidence interval\n\nStatistics means never having to say you’re certain\n– Unknown origin 8\n\nUp to this point in this chapter, I’ve outlined the basics of sampling theory which statisticians rely on to make guesses about population parameters on the basis of a sample of data. As this discussion illustrates, one of the reasons we need all this sampling theory is that every data set leaves us with a some of uncertainty, so our estimates are never going to be perfectly accurate. The thing that has been missing from this discussion is an attempt to quantify the amount of uncertainty that attaches to our estimate. It’s not enough to be able guess that, say, the mean IQ of undergraduate psychology students is \\(115\\) (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a \\(95\\%\\) chance that the true mean lies between \\(109\\) and \\(121\\). The name for this is a confidence interval for the mean.\nArmed with an understanding of sampling distributions, constructing a confidence interval for the mean is actually pretty easy. Here’s how it works. Suppose the true population mean is \\(\\mu\\) and the standard deviation is \\(\\sigma\\). I’ve just finished running my study that has N participants, and the mean IQ among those participants is \\(\\bar{X}\\). We know from our discussion of The central limit theorem that the sampling distribution of the mean is approximately normal. We also know from our discussion of the normal distribution in Section 1.5 that there is a \\(95\\%\\) chance that a normally-distributed quantity will fall within about two standard deviations of the true mean.\nTo be more precise, the more correct answer is that there is a \\(95\\%\\) chance that a normally distributed quantity will fall within \\(1.96\\) standard deviations of the true mean. Next, recall that the standard deviation of the sampling distribution is referred to as the standard error, and the standard error of the mean is written as SEM. When we put all these pieces together, we learn that there is a 95% probability that the sample mean \\(\\bar{X}\\) that we have actually observed lies within \\(1.96\\) standard errors of the population mean.\nOf course, there’s nothing special about the number \\(1.96\\). It just happens to be the multiplier you need to use if you want a \\(95\\%\\) confidence interval. If I’d wanted a \\(70\\%\\) confidence interval, I would have used \\(1.04\\) as the magic number rather than \\(1.96\\).\n[Additional technical detail 9]\nOf course, there’s nothing special about the number 1.96. It just happens to be the multiplier you need to use if you want a 95% confidence interval. If I’d wanted a 70% confidence interval, I would have used 1.04 as the magic number rather than 1.96.\n\n2.5.1 Interpreting a confidence interval\nThe hardest thing about confidence intervals is understanding what they mean. Whenever people first encounter confidence intervals, the first instinct is almost always to say that “there is a 95% probability that the true mean lies inside the confidence interval”. It’s simple and it seems to capture the common sense idea of what it means to say that I am “95% confident”. Unfortunately, it’s not quite right. The intuitive definition relies very heavily on your own personal beliefs about the value of the population mean. I say that I am 95% confident because those are my beliefs. In everyday life that’s perfectly okay, but if you remember back to the the section [What does probability mean?], you’ll notice that talking about personal belief and confidence is a Bayesian idea. However, confidence intervals are not Bayesian tools. Like everything else in this chapter, confidence intervals are frequentist tools, and if you are going to use frequentist methods then it’s not appropriate to attach a Bayesian interpretation to them. If you use frequentist methods, you must adopt frequentist interpretations! Okay, so if that’s not the right answer, what is? Remember what we said about frequentist probability. The only way we are allowed to make “probability statements” is to talk about a sequence of events, and to count up the frequencies of different kinds of events. From that perspective, the interpretation of a 95% confidence interval must have something to do with replication. Specifically, if we replicated the experiment over and over again and computed a 95% confidence interval for each replication, then 95% of those intervals would contain the true mean. More generally, 95% of all confidence intervals constructed using this procedure should contain the true population mean. This idea is illustrated in Figure 2.12, which shows 50 confidence intervals constructed for a “measure 10 IQ scores” experiment (top panel) and another 50 confidence intervals for a “measure 25 IQ scores” experiment (bottom panel). We’d expect that around 95 of our confidence intervals would contain the true population mean, and that’s what we found in Figure 2.12. The critical difference here is that the Bayesian claim makes a probability statement about the population mean (i.e., it refers to our uncertainty about the population mean), which is not allowed under the frequentist interpretation of probability because you can’t “replicate” a population! In the frequentist claim, the population mean is fixed and no probabilistic claims can be made about it. Confidence intervals, however, are repeatable so we can replicate experiments. Therefore a frequentist is allowed to talk about the probability that the confidence interval (a random variable) contains the true mean, but is not allowed to talk about the probability that the true population mean (not a repeatable event) falls within the confidence interval I know that this seems a little pedantic, but it does matter. It matters because the difference in interpretation leads to a difference in the mathematics. There is a Bayesian alternative to confidence intervals, known as credible intervals. In most situations credible intervals are quite similar to confidence intervals, but in other cases they are drastically different. As promised, though, I’ll talk more about the Bayesian perspective in ?sec-Bayesian-statistics.\n\n\n\n\n\nFigure 2.12: 95% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean and the line shows the 95% confidence interval. Most of the 50 confidence intervals do contain the true mean (i.e., 100), but a few - in blue and marked with asterisks - do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people\n\n\n\n\n\n\n2.5.2 Calculating confidence intervals in jamovi\njamovi include a simple way to calculate confidence intervals for the mean as part of the ‘Descriptives’ functionality. Under ‘Descriptives’-‘Statistics’ there is a check box for both the ‘Std. error of Mean’ and ‘Confidence interval for the mean’, so you can use this to find out the 95% confidence interval (which is the default). So, for example, if I load the IQsim.omv file, check ‘Confidence interval for the mean’, I can see the confidence interval associated with the simulated mean IQ: Lower 95% CI = 99.39 and Upper 95% CI = 99.97 So, in our simulated large sample data with N=10,000, the mean IQ score is 99.68 with a 95% CI from 99.39 to 99.97.\nWhen it comes to plotting confidence intervals in jamovi, you can specify that the mean is included as an option in a box plot. Moreover, when we get onto learning about specific statistical tests, for example in ?sec-Comparing-several-means-one-way-ANOVA, we will see that we can also plot confidence intervals as part of the data analysis. That’s pretty cool, so we’ll show you how to do that later on."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#summary",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#summary",
    "title": "2  Estimating unknown quantities from a sample",
    "section": "2.6 Summary",
    "text": "2.6 Summary\nIn this chapter I’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:\n\nBasic ideas about Samples, populations and sampling\nStatistical theory of sampling: The law of large numbers and Sampling distributions and the central limit theorem\nEstimating population parameters. Means and standard deviations\nEstimating a confidence interval\n\nAs always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.\n\n\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan; Company.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard University Press."
  },
  {
    "objectID": "09-Hypothesis-testing.html",
    "href": "09-Hypothesis-testing.html",
    "title": "3  Hypothesis testing",
    "section": "",
    "text": "The process of induction is the process of assuming the simplest law that can be made to harmonize with our experience. This process, however, has no logical foundation but only a psychological one. It is clear that there are no grounds for believing that the simplest course of events will really happen. It is an hypothesis that the sun will rise tomorrow: and this means that we do not know whether it will rise.\n– Ludwig Wittgenstein 1\nIn the last chapter I discussed the ideas behind estimation, which is one of the two “big ideas” in inferential statistics. It’s now time to turn our attention to the other big idea, which is hypothesis testing. In its most abstract form, hypothesis testing is really a very simple idea. The researcher has some theory about the world and wants to determine whether or not the data actually support that theory. However, the details are messy and most people find the theory of hypothesis testing to be the most frustrating part of statistics. The structure of the chapter is as follows. First, I’ll describe how hypothesis testing works in a fair amount of detail, using a simple running example to show you how a hypothesis test is “built”. I’ll try to avoid being too dogmatic while doing so, and focus instead on the underlying logic of the testing procedure.2 Afterwards, I’ll spend a bit of time talking about the various dogmas, rules and heresies that surround the theory of hypothesis testing."
  },
  {
    "objectID": "09-Hypothesis-testing.html#a-menagerie-of-hypotheses",
    "href": "09-Hypothesis-testing.html#a-menagerie-of-hypotheses",
    "title": "3  Hypothesis testing",
    "section": "3.1 A menagerie of hypotheses",
    "text": "3.1 A menagerie of hypotheses\nEventually we all succumb to madness. For me, that day will arrive once I’m finally promoted to full professor. Safely ensconced in my ivory tower, happily protected by tenure, I will finally be able to take leave of my senses (so to speak) and indulge in that most thoroughly unproductive line of psychological research, the search for extrasensory perception (ESP).3\nLet’s suppose that this glorious day has come. My first study is a simple one in which I seek to test whether clairvoyance exists. Each participant sits down at a table and is shown a card by an experimenter. The card is black on one side and white on the other. The experimenter takes the card away and places it on a table in an adjacent room. The card is placed black side up or white side up completely at random, with the randomisation occurring only after the experimenter has left the room with the participant. A second experimenter comes in and asks the participant which side of the card is now facing upwards. It’s purely a one-shot experiment. Each person sees only one card and gives only one answer, and at no stage is the participant actually in contact with someone who knows the right answer. My data set, therefore, is very simple. I have asked the question of N people and some number \\(X\\) of these people have given the correct response. To make things concrete, let’s suppose that I have tested \\(N = 100\\) people and \\(X = 62\\) of these got the answer right. A surprisingly large number, sure, but is it large enough for me to feel safe in claiming I’ve found evidence for ESP? This is the situation where hypothesis testing comes in useful. However, before we talk about how to test hypotheses, we need to be clear about what we mean by hypotheses.\n\n3.1.1 Research hypotheses versus statistical hypotheses\nThe first distinction that you need to keep clear in your mind is between research hypotheses and statistical hypotheses. In my ESP study my overall scientific goal is to demonstrate that clairvoyance exists. In this situation I have a clear research goal: I am hoping to discover evidence for ESP. In other situations I might actually be a lot more neutral than that, so I might say that my research goal is to determine whether or not clairvoyance exists. Regardless of how I want to portray myself, the basic point that I’m trying to convey here is that a research hypothesis involves making a substantive, testable scientific claim. If you are a psychologist then your research hypotheses are fundamentally about psychological constructs. Any of the following would count as research hypotheses:\n\nListening to music reduces your ability to pay attention to other things. This is a claim about the causal relationship between two psychologically meaningful concepts (listening to music and paying attention to things), so it’s a perfectly reasonable research hypothesis.\nIntelligence is related to personality. Like the last one, this is a relational claim about two psychological constructs (intelligence and personality), but the claim is weaker: correlational not causal\nIntelligence is speed of information processing. This hypothesis has a quite different character. It’s not actually a relational claim at all. It’s an ontological claim about the fundamental character of intelligence (and I’m pretty sure this one actually. It’s usually easier to think about how to construct experiments to test research hypotheses of the form “does \\(X\\) affect \\(Y\\)?” than it is to address claims like “what is \\(X\\)?” And in practice what usually happens is that you find ways of testing relational claims that follow from your ontological ones. For instance, if I believe that intelligence is speed of information processing in the brain, my experiments will often involve looking for relationships between measures of intelligence and measures of speed. As a consequence most everyday research questions do tend to be relational in nature, but they’re almost always motivated by deeper ontological questions about the state of nature.\n\nNotice that in practice, my research hypotheses could overlap a lot. My ultimate goal in the ESP experiment might be to test an ontological claim like “ESP exists”, but I might operationally restrict myself to a narrower hypothesis like “Some people can ‘see’ objects in a clairvoyant fashion”. That said, there are some things that really don’t count as proper research hypotheses in any meaningful sense:\n\nLove is a battlefield. This is too vague to be testable. Whilst it’s okay for a research hypothesis to have a degree of vagueness to it, it has to be possible to operationalise your theoretical ideas. Maybe I’m just not creative enough to see it, but I can’t see how this can be converted into any concrete research design. If that’s true then this isn’t a scientific research hypothesis, it’s a pop song. That doesn’t mean it’s not interesting. A lot of deep questions that humans have fall into this category. Maybe one day science will be able to construct testable theories of love, or to test to see if God exists, and so on. But right now we can’t, and I wouldn’t bet on ever seeing a satisfying scientific approach to either.\nThe first rule of tautology club is the first rule of tautology club. This is not a substantive claim of any kind. It’s true by definition. No conceivable state of nature could possibly be inconsistent with this claim. We say that this is an unfalsifiable hypothesis, and as such it is outside the domain of science. Whatever else you do in science your claims must have the possibility of being wrong.\nMore people in my experiment will say “yes” than “no”. This one fails as a research hypothesis because it’s a claim about the data set, not about the psychology (unless of course your actual research question is whether people have some kind of “yes” bias!). Actually, this hypothesis is starting to sound more like a statistical hypothesis than a research hypothesis.\n\nAs you can see, research hypotheses can be somewhat messy at times and ultimately they are scientific claims. Statistical hypotheses are neither of these two things. Statistical hypotheses must be mathematically precise and they must correspond to specific claims about the characteristics of the data generating mechanism (i.e., the “population”). Even so, the intent is that statistical hypotheses bear a clear relationship to the substantive research hypotheses that you care about! For instance, in my ESP study my research hypothesis is that some people are able to see through walls or whatever. What I want to do is to “map” this onto a statement about how the data were generated. So let’s think about what that statement would be. The quantity that I’m interested in within the experiment is \\(P(correct)\\), the true-but-unknown probability with which the participants in my experiment answer the question correctly. Let’s use the Greek letter \\(\\theta\\) (theta) to refer to this probability. Here are four different statistical hypotheses:\n\nIf ESP doesn’t exist and if my experiment is well designed then my participants are just guessing. So I should expect them to get it right half of the time and so my statistical hypothesis is that the true probability of choosing correctly is \\(\\theta=0.5\\) .\nAlternatively, suppose ESP does exist and participants can see the card. If that’s true people will perform better than chance and the statistical hypothesis is that \\(\\theta > 0.5\\).\nA third possibility is that ESP does exist, but the colours are all reversed and people don’t realise it (okay, that’s wacky, but you never know). If that’s how it works then you’d expect people’s performance to be below chance. This would correspond to a statistical hypothesis that \\(\\theta < 0.5\\).\nFinally, suppose ESP exists but I have no idea whether people are seeing the right colour or the wrong one. In that case the only claim I could make about the data would be that the probability of making the correct answer is not equal to 0.5. This corresponds to the statistical hypothesis that \\(\\theta \\neq 0.5\\).\n\nAll of these are legitimate examples of a statistical hypothesis because they are statements about a population parameter and are meaningfully related to my experiment.\nWhat this discussion makes clear, I hope, is that when attempting to construct a statistical hypothesis test the researcher actually has two quite distinct hypotheses to consider. First, he or she has a research hypothesis (a claim about psychology), and this then corresponds to a statistical hypothesis (a claim about the data generating population). In my ESP example these might be as shown in Table 3.1.\n\n\n\n\nTable 3.1:  Research and statistical hypotheses \n\nDani's research hypothesis:\"ESP exists\"\n\nDani's statistical hypothesis:\\( \\theta \\neq 0.5 \\)\n\n\n\n\n\nAnd a key thing to recognise is this. A statistical hypothesis test is a test of the statistical hypothesis, not the research hypothesis. If your study is badly designed then the link between your research hypothesis and your statistical hypothesis is broken. To give a silly example, suppose that my ESP study was conducted in a situation where the participant can actually see the card reflected in a window. If that happens I would be able to find very strong evidence that \\(\\theta \\neq 0.5\\), but this would tell us nothing about whether “ESP exists”.\n\n\n3.1.2 Null hypotheses and alternative hypotheses\nSo far, so good. I have a research hypothesis that corresponds to what I want to believe about the world, and I can map it onto a statistical hypothesis that corresponds to what I want to believe about how the data were generated. It’s at this point that things get somewhat counter-intuitive for a lot of people. Because what I’m about to do is invent a new statistical hypothesis (the “null” hypothesis, \\(H_0\\) ) that corresponds to the exact opposite of what I want to believe, and then focus exclusively on that almost to the neglect of the thing I’m actually interested in (which is now called the “alternative” hypothesis, H1). In our ESP example, the null hypothesis is that \\(\\theta = 0.5\\), since that’s what we’d expect if ESP didn’t exist. My hope, of course, is that ESP is totally real and so the alternative to this null hypothesis is \\(\\theta \\neq 0.5\\). In essence, what we’re doing here is dividing up the possible values of \\(\\theta\\) into two groups: those values that I really hope aren’t true (the null), and those values that I’d be happy with if they turn out to be right (the alternative). Having done so, the important thing to recognise is that the goal of a hypothesis test is not to show that the alternative hypothesis is (probably) true. The goal is to show that the null hypothesis is (probably) false. Most people find this pretty weird.\nThe best way to think about it, in my experience, is to imagine that a hypothesis test is a criminal trial4, the trial of the null hypothesis. The null hypothesis is the defendant, the researcher is the prosecutor, and the statistical test itself is the judge. Just like a criminal trial, there is a presumption of innocence. The null hypothesis is deemed to be true unless you, the researcher, can prove beyond a reasonable doubt that it is false. You are free to design your experiment however you like (within reason, obviously!) and your goal when doing so is to maximise the chance that the data will yield a conviction for the crime of being false. The catch is that the statistical test sets the rules of the trial and those rules are designed to protect the null hypothesis, specifically to ensure that if the null hypothesis is actually true the chances of a false conviction are guaranteed to be low. This is pretty important. After all, the null hypothesis doesn’t get a lawyer, and given that the researcher is trying desperately to prove it to be false someone has to protect it."
  },
  {
    "objectID": "09-Hypothesis-testing.html#two-types-of-errors",
    "href": "09-Hypothesis-testing.html#two-types-of-errors",
    "title": "3  Hypothesis testing",
    "section": "3.2 Two types of errors",
    "text": "3.2 Two types of errors\nBefore going into details about how a statistical test is constructed it’s useful to understand the philosophy behind it. I hinted at it when pointing out the similarity between a null hypothesis test and a criminal trial, but I should now be explicit. Ideally, we would like to construct our test so that we never make any errors. Unfortunately, since the world is messy, this is never possible. Sometimes you’re just really unlucky. For instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like very strong evidence for a conclusion that the coin is biased, but of course there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life we always have to accept that there’s a chance that we made a mistake. As a consequence the goal behind statistical hypothesis testing is not to eliminate errors, but to minimise them.\nAt this point, we need to be a bit more precise about what we mean by “errors”. First, let’s state the obvious. It is either the case that the null hypothesis is true or that it is false, and our test will either retain the null hypothesis or reject it.5 So, as Table 3.2 illustrates, after we run the test and make our choice one of four things might have happened:\n\n\n\n\nTable 3.2:  Null hypothesis statistical testing (NHST) \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is truecorrect decisionerror (type I)\n\n\\( H_0 \\) is falseerror (type II)correct decision\n\n\n\n\n\nAs a consequence there are actually two different types of error here. If we reject a null hypothesis that is actually true then we have made a type I error. On the other hand, if we retain the null hypothesis when it is in fact false then we have made a type II error.\nRemember how I said that statistical testing was kind of like a criminal trial? Well, I meant it. A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. All of the evidential rules are (in theory, at least) designed to ensure that there’s (almost) no chance of wrongfully convicting an innocent defendant. The trial is designed to protect the rights of a defendant, as the English jurist William Blackstone famously said, it is “better that ten guilty persons escape than that one innocent suffer.” In other words, a criminal trial doesn’t treat the two types of error in the same way. Punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same. The single most important design principle of the test is to control the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted \\(\\alpha\\), is called the significance level of the test. And I’ll say it again, because it is so central to the whole set-up, a hypothesis test is said to have significance level \\(\\alpha\\) if the type I error rate is no larger than \\(\\alpha\\).\nSo, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by \\(\\beta\\). However, it’s much more common to refer to the power of the test, that is the probability with which we reject a null hypothesis when it really is false, which is \\(1 - \\beta\\). To help keep this straight, here’s the same table again but with the relevant numbers added (Table 3.3):\n\n\n\n\nTable 3.3:  Null hypothesis statistical testing (NHST) - additional detail \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is true1-\\( \\alpha \\) (probability of correct retention)\\(\\alpha\\)  (type I error rate)\n\n\\( H_0 \\) is false\\(\\beta\\) (type II error rate)\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\nA “powerful” hypothesis test is one that has a small value of \\(\\beta\\), while still keeping \\(\\alpha\\) fixed at some (small) desired level. By convention, scientists make use of three different \\(\\alpha\\) levels: \\(.05\\), \\(.01\\) and \\(.001\\). Notice the asymmetry here; the tests are designed to ensure that the \\(\\alpha\\) level is kept small but there’s no corresponding guarantee regarding \\(\\beta\\). We’d certainly like the type II error rate to be small and we try to design tests that keep it small, but this is typically secondary to the overwhelming need to control the type I error rate. As Blackstone might have said if he were a statistician, it is “better to retain 10 false null hypotheses than to reject a single true one”. To be honest, I don’t know that I agree with this philosophy. There are situations where I think it makes sense, and situations where I think it doesn’t, but that’s neither here nor there. It’s how the tests are built."
  },
  {
    "objectID": "09-Hypothesis-testing.html#test-statistics-and-sampling-distributions",
    "href": "09-Hypothesis-testing.html#test-statistics-and-sampling-distributions",
    "title": "3  Hypothesis testing",
    "section": "3.3 Test statistics and sampling distributions",
    "text": "3.3 Test statistics and sampling distributions\nAt this point we need to start talking specifics about how a hypothesis test is constructed. To that end, let’s return to the ESP example. Let’s ignore the actual data that we obtained, for the moment, and think about the structure of the experiment. Regardless of what the actual numbers are, the form of the data is that \\(X\\) out of \\(N\\) people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true, that ESP doesn’t exist and the true probability that anyone picks the correct colour is exactly \\(\\theta = 0.5\\). What would we expect the data to look like? Well, obviously we’d expect the proportion of people who make the correct response to be pretty close to \\(50\\%\\). Or, to phrase this in more mathematical terms, we’d say that \\(\\frac{X}{N}\\) is approximately \\(0.5\\). Of course, we wouldn’t expect this fraction to be exactly \\(0.5\\). If, for example, we tested \\(N = 100\\) people and \\(X = 53\\) of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if \\(X = 99\\) of our participants got the question right then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only \\(X = 3\\) people got the answer right we’d be similarly confident that the null was wrong. Let’s be a little more technical about this. We have a quantity \\(X\\) that we can calculate by looking at our data. After looking at the value of \\(X\\) we make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a test statistic.\nHaving chosen a test statistic, the next step is to state precisely which values of the test statistic would cause is to reject the null hypothesis, and which values would cause us to keep it. In order to do so we need to determine what the sampling distribution of the test statistic would be if the null hypothesis were actually true (we talked about sampling distributions earlier in Section 2.3.1. Why do we need this? Because this distribution tells us exactly what values of X our null hypothesis would lead us to expect. And, therefore, we can use this distribution as a tool for assessing how closely the null hypothesis agrees with our data.\nHow do we actually determine the sampling distribution of the test statistic? For a lot of hypothesis tests this step is actually quite complicated, and later on in the book you’ll see me being slightly evasive about it for some of the tests (some of them I don’t even understand myself). However, sometimes it’s very easy. And, fortunately for us, our ESP example provides us with one of the easiest cases. Our population parameter \\(\\theta\\) is just the overall probability that people respond correctly when asked the question, and our test statistic \\(X\\) is the count of the number of people who did so out of a sample size of N. We’ve seen a distribution like this before, in Section 1.4, and that’s exactly what the binomial distribution describes! So, to use the notation and terminology that I introduced in that section, we would say that the null hypothesis predicts that \\(X\\) is binomially distributed, which is written\n\\[X \\sim Binomial(\\theta,N)\\]\nSince the null hypothesis states that \\(\\theta = 0.5\\) and our experiment has \\(N = 100\\) people, we have the sampling distribution we need. This sampling distribution is plotted in Figure 3.1. No surprises really, the null hypothesis says that \\(X = 50\\) is the most likely outcome, and it says that we’re almost certain to see somewhere between \\(40\\) and \\(60\\) correct responses.\n\n\n\n\n\nFigure 3.1: The sampling distribution for our test statistic \\(X\\) when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is \\(\\theta = .5\\), the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60"
  },
  {
    "objectID": "09-Hypothesis-testing.html#making-decisions",
    "href": "09-Hypothesis-testing.html#making-decisions",
    "title": "3  Hypothesis testing",
    "section": "3.4 Making decisions",
    "text": "3.4 Making decisions\nOkay, we’re very close to being finished. We’ve constructed a test statistic \\((X)\\) and we chose this test statistic in such a way that we’re pretty confident that if \\(X\\) is close to \\(\\frac{N}{2}\\) then we should retain the null, and if not we should reject it. The question that remains is this. Exactly which values of the test statistic should we associate with the null hypothesis, and exactly which values go with the alternative hypothesis? In my ESP study, for example, I’ve observed a value of \\(X = 62\\). What decision should I make? Should I choose to believe the null hypothesis or the alternative hypothesis?\n\n3.4.1 Critical regions and critical values\nTo answer this question we need to introduce the concept of a critical region for the test statistic X. The critical region of the test corresponds to those values of X that would lead us to reject null hypothesis (which is why the critical region is also sometimes called the rejection region). How do we find this critical region? Well, let’s consider what we know:\n\n\\(X\\) should be very big or very small in order to reject the null hypothesis\nIf the null hypothesis is true, the sampling distribution of \\(X\\) is \\(Binomial(0.5, N)\\)\nIf \\(\\alpha = .05\\), the critical region must cover 5% of this sampling distribution.\n\nIt’s important to make sure you understand this last point. The critical region corresponds to those values of \\(X\\) for which we would reject the null hypothesis, and the sampling distribution in question describes the probability that we would obtain a particular value of \\(X\\) if the null hypothesis were actually true. Now, let’s suppose that we chose a critical region that covers \\(20\\%\\) of the sampling distribution, and suppose that the null hypothesis is actually true. What would be the probability of incorrectly rejecting the null? The answer is of course \\(20\\%\\). And, therefore, we would have built a test that had an α level of \\(0.2\\). If we want \\(\\alpha = .05\\), the critical region is only allowed to cover 5% of the sampling distribution of our test statistic.\nAs it turns out those three things uniquely solve the problem. Our critical region consists of the most extreme values, known as the tails of the distribution. This is illustrated in Figure 3.2. If we want \\(\\alpha = .05\\) then our critical regions correspond to \\(X \\leq 40\\) and \\(X \\geq 60\\).6 That is, if the number of people saying “true” is between 41 and 59, then we should retain the null hypothesis. If the number is between \\(0\\) to \\(40\\), or between \\(60\\) to \\(100\\), then we should reject the null hypothesis. The numbers \\(40\\) and \\(60\\) are often referred to as the critical values since they define the edges of the critical region\n\n\n\n\n\nFigure 3.2: The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of \\(\\alpha = .05\\). The plot shows the sampling distribution of \\(X\\) under the null hypothesis (i.e., same as Figure 3.1) . The grey bars correspond to those values of \\(X\\) for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of \\(X\\) for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both \\(\\theta < .5\\) and \\(\\theta > .5\\), the critical region covers both tails of the distribution. To ensure an \\(\\alpha\\) level of \\(.05\\), we need to ensure that each of the two regions encompasses \\(2.5\\%\\) of the sampling distribution\n\n\n\n\nAt this point, our hypothesis test is essentially complete:\n\nWe choose an α level (e.g., \\(\\alpha = .05\\));\nCome up with some test statistic (e.g., \\(X\\)) that does a good job (in some meaningful sense) of comparing \\(H_0\\) to \\(H_1\\);\nFigure out the sampling distribution of the test statistic on the assumption that the null hypothesis is true (in this case, binomial); and then\nCalculate the critical region that produces an appropriate α level (0-40 and 60-100).\n\nAll that we have to do now is calculate the value of the test statistic for the real data (e.g., X = 62) and then compare it to the critical values to make our decision. Since 62 is greater than the critical value of 60 we would reject the null hypothesis. Or, to phrase it slightly differently, we say that the test has produced a statistically significant result.\n\n\n3.4.2 A note on statistical “significance”\n\nLike other occult techniques of divination, the statistical method has a private jargon deliberately contrived to obscure its methods from non-practitioners.\n– Attributed to G. O. Ashley 7\n\nA very brief digression is in order at this point, regarding the word “significant”. The concept of statistical significance is actually a very simple one, but has a very unfortunate name. If the data allow us to reject the null hypothesis, we say that “the result is statistically significant”, which is often shortened to “the result is significant”. This terminology is rather old and dates back to a time when “significant” just meant something like “indicated”, rather than its modern meaning which is much closer to “important”. As a result, a lot of modern readers get very confused when they start learning statistics because they think that a “significant result” must be an important one. It doesn’t mean that at all. All that “statistically significant” means is that the data allowed us to reject a null hypothesis. Whether or not the result is actually important in the real world is a very different question, and depends on all sorts of other things.\n\n\n3.4.3 The difference between one sided and two sided tests\nThere’s one more thing I want to point out about the hypothesis test that I’ve just constructed. If we take a moment to think about the statistical hypotheses I’ve been using, \\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] we notice that the alternative hypothesis covers both the possibility that \\(\\theta < .5\\) and the possibility that \\(\\theta \\> .5.\\) This makes sense if I really think that ESP could produce either better-than chance performance or worse-than-chance performance (and there are some people who think that). In statistical language this is an example of a two-sided test. It’s called this because the alternative hypothesis covers the area on both “sides” of the null hypothesis, and as a consequence the critical region of the test covers both tails of the sampling distribution (2.5% on either side if α = .05), as illustrated earlier in Figure 3.2. However, that’s not the only possibility. I might only be willing to believe in ESP if it produces better than chance performance. If so, then my alternative hypothesis would only covers the possibility that \\(\\theta > .5\\), and as a consequence the null hypothesis now becomes \\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta > 0.5\\] When this happens, we have what’s called a one-sided test and the critical region only covers one tail of the sampling distribution. This is illustrated in Figure 3.3.\n\n\n\n\n\nFigure 3.3: The critical region for a one sided test. In this case, the alternative hypothesis is that \\(\\theta \\geq .5\\) so we would only reject the null hypothesis for large values of \\(X\\). As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper \\(5\\%\\) of the distribution. Contrast this to the two-sided version in Figure 3.2"
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-The-p-value-of-a-test",
    "href": "09-Hypothesis-testing.html#sec-The-p-value-of-a-test",
    "title": "3  Hypothesis testing",
    "section": "3.5 The p value of a test",
    "text": "3.5 The p value of a test\nIn one sense, our hypothesis test is complete. We’ve constructed a test statistic, figured out its sampling distribution if the null hypothesis is true, and then constructed the critical region for the test. Nevertheless, I’ve actually omitted the most important number of all, the p value. It is to this topic that we now turn. There are two somewhat different ways of interpreting a p value, one proposed by Sir Ronald Fisher and the other by Jerzy Neyman. Both versions are legitimate, though they reflect very different ways of thinking about hypothesis tests. Most introductory textbooks tend to give Fisher’s version only, but I think that’s a bit of a shame. To my mind, Neyman’s version is cleaner and actually better reflects the logic of the null hypothesis test. You might disagree though, so I’ve included both. I’ll start with Neyman’s version.\n\n3.5.1 A softer view of decision making\nOne problem with the hypothesis testing procedure that I’ve described is that it makes no distinction at all between a result that is “barely significant” and those that are “highly significant”. For instance, in my ESP study the data I obtained only just fell inside the critical region, so I did get a significant effect but it was a pretty near thing. In contrast, suppose that I’d run a study in which \\(X = 97\\) out of my \\(N = 100\\) participants got the answer right. This would obviously be significant too but my a much larger margin, such that there’s really no ambiguity about this at all. The procedure that I have already described makes no distinction between the two. If I adopt the standard convention of allowing \\(\\alpha = .05\\) as my acceptable Type I error rate, then both of these are significant results.\nThis is where the p value comes in handy. To understand how it works, let’s suppose that we ran lots of hypothesis tests on the same data set, but with a different value of α in each case. When we do that for my original ESP data what we’d get is something like Table 3.4.\n\n\n\n\nTable 3.4:  Rejecting the NH at different levels of alpha \n\nValue of \\( \\alpha \\)0.050.040.030.020.01\n\nReject the null?YesYesYesNoNo\n\n\n\n\n\nWhen we test the ESP data (\\(X = 62\\) successes out of \\(N = 100\\) observations), using \\(\\alpha\\) levels of \\(.03\\) and above, we’d always find ourselves rejecting the null hypothesis. For \\(\\alpha\\) levels of \\(.02\\) and below we always end up retaining the null hypothesis. Therefore, somewhere between \\(.02\\) and \\(.03\\) there must be a smallest value of \\(\\alpha\\) that would allow us to reject the null hypothesis for this data. This is the \\(p\\) value. As it turns out the ESP data has \\(p = .021\\). In short, \\(p\\) is defined to be the smallest Type I error rate (\\(\\alpha\\)) that you have to be willing to tolerate if you want to reject the null hypothesis.\nIf it turns out that p describes an error rate that you find intolerable, then you must retain the null. If you’re comfortable with an error rate equal to \\(p\\), then it’s okay to reject the null hypothesis in favour of your preferred alternative.\nIn effect, \\(p\\) is a summary of all the possible hypothesis tests that you could have run, taken across all possible α values. And as a consequence it has the effect of “softening” our decision process. For those tests in which p ď α you would have rejected the null hypothesis, whereas for those tests in which p ą α you would have retained the null. In my ESP study I obtained \\(X = 62\\) and as a consequence I’ve ended up with \\(p = .021\\). So the error rate I have to tolerate is \\(2.1\\%\\). In contrast, suppose my experiment had yielded \\(X = 97\\). What happens to my p value now? This time it’s shrunk to \\(p = 1.36\\) x \\(10^{-25}\\), which is a tiny, tiny8 Type I error rate. For this second case I would be able to reject the null hypothesis with a lot more confidence, because I only have to be “willing” to tolerate a type I error rate of about \\(1\\) in \\(10\\) trillion trillion in order to justify my decision to reject.\n\n\n3.5.2 The probability of extreme data\nThe second definition of the p-value comes from Sir Ronald Fisher, and it’s actually this one that you tend to see in most introductory statistics textbooks. Notice how, when I constructed the critical region, it corresponded to the tails (i.e., extreme values) of the sampling distribution? That’s not a coincidence, almost all “good” tests have this characteristic (good in the sense of minimising our type II error rate, \\(\\beta\\)). The reason for that is that a good critical region almost always corresponds to those values of the test statistic that are least likely to be observed if the null hypothesis is true. If this rule is true, then we can define the p-value as the probability that we would have observed a test statistic that is at least as extreme as the one we actually did get. In other words, if the data are extremely implausible according to the null hypothesis, then the null hypothesis is probably wrong.\n\n\n3.5.3 A common mistake\nOkay, so you can see that there are two rather different but legitimate ways to interpret the \\(p\\) value, one based on Neyman’s approach to hypothesis testing and the other based on Fisher’s. Unfortunately, there is a third explanation that people sometimes give, especially when they’re first learning statistics, and it is absolutely and completely wrong. This mistaken approach is to refer to the \\(p\\) value as “the probability that the null hypothesis is true”. It’s an intuitively appealing way to think, but it’s wrong in two key respects. First, null hypothesis testing is a frequentist tool and the frequentist approach to probability does not allow you to assign probabilities to the null hypothesis. According to this view of probability, the null hypothesis is either true or it is not, it cannot have a “\\(5\\%\\) chance” of being true. Second, even within the Bayesian approach, which does let you assign probabilities to hypotheses, the p value would not correspond to the probability that the null is true. This interpretation is entirely inconsistent with the mathematics of how the p value is calculated. Put bluntly, despite the intuitive appeal of thinking this way, there is no justification for interpreting a \\(p\\) value this way. Never do it."
  },
  {
    "objectID": "09-Hypothesis-testing.html#reporting-the-results-of-a-hypothesis-test",
    "href": "09-Hypothesis-testing.html#reporting-the-results-of-a-hypothesis-test",
    "title": "3  Hypothesis testing",
    "section": "3.6 Reporting the results of a hypothesis test",
    "text": "3.6 Reporting the results of a hypothesis test\nWhen writing up the results of a hypothesis test there’s usually several pieces of information that you need to report, but it varies a fair bit from test to test. Throughout the rest of the book I’ll spend a little time talking about how to report the results of different tests (see ?sec-How-to-report-the-results-of-a-test for a particularly detailed example, so that you can get a feel for how it’s usually done. However, regardless of what test you’re doing, the one thing that you always have to do is say something about the \\(p\\) value and whether or not the outcome was significant.\nThe fact that you have to do this is unsurprising, it’s the whole point of doing the test. What might be surprising is the fact that there is some contention over exactly how you’re supposed to do it. Leaving aside those people who completely disagree with the entire framework underpinning null hypothesis testing, there’s a certain amount of tension that exists regarding whether or not to report the exact \\(p\\) value that you obtained, or if you should state only that \\(p < \\alpha\\) for a significance level that you chose in advance (e.g., \\(p < .05\\)).\n\n3.6.1 The issue\nTo see why this is an issue, the key thing to recognise is that p values are terribly convenient. In practice, the fact that we can compute a p value means that we don’t actually have to specify any \\(\\alpha\\) level at all in order to run the test. Instead, what you can do is calculate your p value and interpret it directly. If you get \\(p = .062\\), then it means that you’d have to be willing to tolerate a Type I error rate of \\(6.2\\%\\) to justify rejecting the null. If you personally find \\(6.2\\%\\) intolerable then you retain the null. Therefore, the argument goes, why don’t we just report the actual \\(p\\) value and let the reader make up their own minds about what an acceptable Type I error rate is? This approach has the big advantage of “softening” the decision making process. In fact, if you accept the Neyman definition of the p value, that’s the whole point of the p value. We no longer have a fixed significance level of \\(\\alpha = .05\\) as a bright line separating “accept” from “reject” decisions, and this removes the rather pathological problem of being forced to treat \\(p = .051\\) in a fundamentally different way to \\(p = .049\\).\nThis flexibility is both the advantage and the disadvantage to the \\(p\\) value. The reason why a lot of people don’t like the idea of reporting an exact \\(p\\) value is that it gives the researcher a bit too much freedom. In particular, it lets you change your mind about what error tolerance you’re willing to put up with after you look at the data. For instance, consider my ESP experiment. Suppose I ran my test and ended up with a \\(p\\) value of \\(.09\\). Should I accept or reject? Now, to be honest, I haven’t yet bothered to think about what level of Type I error I’m “really” willing to accept. I don’t have an opinion on that topic. But I do have an opinion about whether or not ESP exists, and I definitely have an opinion about whether my research should be published in a reputable scientific journal. And amazingly, now that I’ve looked at the data I’m starting to think that a \\(9\\%\\) error rate isn’t so bad, especially when compared to how annoying it would be to have to admit to the world that my experiment has failed. So, to avoid looking like I just made it up after the fact, I now say that my \\(\\alpha\\) is .1, with the argument that a \\(10\\%\\) type I error rate isn’t too bad and at that level my test is significant! I win.\nIn other words, the worry here is that I might have the best of intentions, and be the most honest of people, but the temptation to just “shade” things a little bit here and there is really, really strong. As anyone who has ever run an experiment can attest, it’s a long and difficult process and you often get very attached to your hypotheses. It’s hard to let go and admit the experiment didn’t find what you wanted it to find. And that’s the danger here. If we use the “raw” p-value, people will start interpreting the data in terms of what they want to believe, not what the data are actually saying and, if we allow that, why are we even bothering to do science at all? Why not let everyone believe whatever they like about anything, regardless of what the facts are? Okay, that’s a bit extreme, but that’s where the worry comes from. According to this view, you really must specify your \\(\\alpha\\) value in advance and then only report whether the test was significant or not. It’s the only way to keep ourselves honest\n\n\n\n\nTable 3.5:  Typical translations of p value levels \n\nUsual notationSignif. starsEnglish translationThe null is...\n\np > .05The test wasn't significantRetained\n\np < .05*The test was significant at \\( \\alpha \\) = .05 but not at \\( \\alpha \\) = .01 or \\( \\alpha \\) = .001.Rejected\n\np < .01**The test was significant at \\( \\alpha \\) = .05  and \\( \\alpha \\) = .01 but not at \\( \\alpha \\) = .001.Rejected\n\np < .001***The test was significant at all levelsRejected\n\n\n\n\n\n\n\n3.6.2 Two proposed solutions\nIn practice, it’s pretty rare for a researcher to specify a single α level ahead of time. Instead, the convention is that scientists rely on three standard significance levels: \\(.05\\), \\(.01\\) and \\(.001\\). When reporting your results, you indicate which (if any) of these significance levels allow you to reject the null hypothesis. This is summarised in Table 3.5. This allows us to soften the decision rule a little bit, since \\(p < .01\\) implies that the data meet a stronger evidential standard than \\(p < .05\\) would. Nevertheless, since these levels are fixed in advance by convention, it does prevent people choosing their α level after looking at the data\nNevertheless, quite a lot of people still prefer to report exact p values. To many people, the advantage of allowing the reader to make up their own mind about how to interpret p = .06 outweighs any disadvantages. In practice, however, even among those researchers who prefer exact p values it is quite common to just write \\(p < .001\\) instead of reporting an exact value for small p. This is in part because a lot of software doesn’t actually print out the p value when it’s that small (e.g., SPSS just writes \\(p = .000\\) whenever \\(p < .001\\)), and in part because a very small p value can be kind of misleading. The human mind sees a number like .0000000001 and it’s hard to suppress the gut feeling that the evidence in favour of the alternative hypothesis is a near certainty. In practice however, this is usually wrong. Life is a big, messy, complicated thing, and every statistical test ever invented relies on simplifications, approximations and assumptions. As a consequence, it’s probably not reasonable to walk away from any statistical analysis with a feeling of confidence stronger than \\(p < .001\\) implies. In other words, \\(p < .001\\) is really code for “as far as this test is concerned, the evidence is overwhelming.”\nIn light of all this, you might be wondering exactly what you should do. There’s a fair bit of contradictory advice on the topic, with some people arguing that you should report the exact p value, and other people arguing that you should use the tiered approach illustrated in Table 3.1. As a result, the best advice I can give is to suggest that you look at papers/reports written in your field and see what the convention seems to be. If there doesn’t seem to be any consistent pattern, then use whichever method you prefer."
  },
  {
    "objectID": "09-Hypothesis-testing.html#running-the-hypothesis-test-in-practice",
    "href": "09-Hypothesis-testing.html#running-the-hypothesis-test-in-practice",
    "title": "3  Hypothesis testing",
    "section": "3.7 Running the hypothesis test in practice",
    "text": "3.7 Running the hypothesis test in practice\nAt this point some of you might be wondering if this is a “real” hypothesis test, or just a toy example that I made up. It’s real. In the previous discussion I built the test from first principles, thinking that it was the simplest possible problem that you might ever encounter in real life. However, this test already exists. It’s called the binomial test, and it’s implemented by jamovi as one of the statistical analyses available when you hit the ‘Frequencies’ button. To test the null hypothesis that the response probability is one-half \\(p = .5\\),9 and using data in which \\(x =62\\) of \\(n = 100\\) people made the correct response, available in the binomialtest.omv data file, we get the results shown in Figure 3.4.\n\n\n\n\n\nFigure 3.4: Binomial test analysis and results in jamovi\n\n\n\n\nRight now, this output looks pretty unfamiliar to you, but you can see that it’s telling you more or less the right things. Specifically, the p-value of \\(0.02\\) is less than the usual choice of \\(\\alpha = .05\\), so you can reject the null. We’ll talk a lot more about how to read this sort of output as we go along, and after a while you’ll hopefully find it quite easy to read and understand."
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-Effect-size-sample-size-and-power",
    "href": "09-Hypothesis-testing.html#sec-Effect-size-sample-size-and-power",
    "title": "3  Hypothesis testing",
    "section": "3.8 Effect size, sample size and power",
    "text": "3.8 Effect size, sample size and power\nIn previous sections I’ve emphasised the fact that the major design principle behind statistical hypothesis testing is that we try to control our Type I error rate. When we fix \\(\\alpha = .05\\) we are attempting to ensure that only \\(5\\%\\) of true null hypotheses are incorrectly rejected. However, this doesn’t mean that we don’t care about Type II errors. In fact, from the researcher’s perspective, the error of failing to reject the null when it is actually false is an extremely annoying one. With that in mind, a secondary goal of hypothesis testing is to try to minimise \\(\\beta\\), the Type II error rate, although we don’t usually talk in terms of minimising Type II errors. Instead, we talk about maximising the power of the test. Since power is defined as \\(1 - \\beta\\), this is the same thing.\n\n3.8.1 The power function\n\n\n\n\n\nFigure 3.5: Sampling distribution under the alternative hypothesis for a population parameter value of \\(\\theta = 0.55\\). A reasonable proportion of the distribution lies in the rejection region\n\n\n\n\nLet’s take a moment to think about what a Type II error actually is. A Type II error occurs when the alternative hypothesis is true, but we are nevertheless unable to reject the null hypothesis. Ideally, we’d be able to calculate a single number \\(\\beta\\) that tells us the Type II error rate, in the same way that we can set \\(\\alpha = .05\\) for the Type I error rate. Unfortunately, this is a lot trickier to do. To see this, notice that in my ESP study the alternative hypothesis actually corresponds to lots of possible values of \\(\\theta\\). In fact, the alternative hypothesis corresponds to every value of \\(\\theta\\) except 0.5. Let’s suppose that the true probability of someone choosing the correct response is 55% (i.e., \\(\\theta = .55\\)). If so, then the true sampling distribution for \\(X\\) is not the same one that the null hypothesis predicts, as the most likely value for \\(X\\) is now \\(55\\) out of 100. Not only that, the whole sampling distribution has now shifted, as shown in Figure 3.5. The critical regions, of course, do not change. By definition the critical regions are based on what the null hypothesis predicts. What we’re seeing in this figure is the fact that when the null hypothesis is wrong, a much larger proportion of the sampling distribution distribution falls in the critical region. And of course that’s what should happen. The probability of rejecting the null hypothesis is larger when the null hypothesis is actually false! However \\(\\theta = .55\\) is not the only possibility consistent with the alternative hypothesis. Let’s instead suppose that the true value of \\(\\theta\\) is actually \\(0.7\\). What happens to the sampling distribution when this occurs? The answer, shown in Figure 3.6, is that almost the entirety of the sampling distribution has now moved into the critical region. Therefore, if \\(\\theta = 0.7\\), the probability of us correctly rejecting the null hypothesis (i.e., the power of the test) is much larger than if \\(\\theta = 0.55\\). In short, while \\(\\theta = .55\\) and \\(\\theta = .70\\) are both part of the alternative hypothesis, the Type II error rate is different.\n\n\n\n\n\nFigure 3.6: Sampling distribution under the alternative hypothesis for a population parameter value of \\(\\theta = 0.70\\). Almost all of the distribution lies in the rejection region.\n\n\n\n\nWhat all this means is that the power of a test (i.e., \\(1 - \\beta\\)) depends on the true value of \\(\\theta\\). To illustrate this, I’ve calculated the expected probability of rejecting the null hypothesis for all values of \\(\\theta\\), and plotted it in Figure 3.7. This plot describes what is usually called the power function of the test. It’s a nice summary of how good the test is, because it actually tells you the power \\((1 - \\beta\\)) for all possible values of \\(\\theta\\). As you can see, when the true value of \\(\\theta\\) is very close to \\(0.5\\), the power of the test drops very sharply, but when it is further away, the power is large.\n\n\n\n\n\nFigure 3.7: The probability that we will reject the null hypothesis, plotted as a function of the true value of \\(\\theta\\). Obviously, the test is more powerful (greater chance of correct rejection) if the true value of \\(\\theta\\) is very different from the value that the null hypothesis specifies (i.e., \\(\\theta = .5\\) ). Notice that when \\(\\theta\\) actually is equal to \\(.5\\) (plotted as a black dot), the null hypothesis is in fact true and rejecting the null hypothesis in this instance would be a Type I error\n\n\n\n\n\n\n3.8.2 The power function\n\nSince all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned with mice when there are tigers abroad\n- George Box (Box 1976, p. 792)\n\nThe plot shown in Figure 3.7 captures a fairly basic point about hypothesis testing. If the true state of the world is very different from what the null hypothesis predicts then your power will be very high, but if the true state of the world is similar to the null (but not identical) then the power of the test is going to be very low. Therefore, it’s useful to be able to have some way of quantifying how “similar” the true state of the world is to the null hypothesis. A statistic that does this is called a measure of effect size (e.g., Cohen (1988); Ellis (2010)). Effect size is defined slightly differently in different contexts (and so this section just talks in general terms) but the qualitative idea that it tries to capture is always the same (see e.g. Table 3.6). How big is the difference between the true population parameters and the parameter values that are assumed by the null hypothesis? In our ESP example, if we let \\(\\theta_0 = 0.5\\) denote the value assumed by the null hypothesis and let \\(\\theta\\) denote the true value, then a simple measure of effect size could be something like the difference between the true value and null (i.e., \\(\\theta - \\theta_0\\)), or possibly just the magnitude of this difference, \\(abs(\\theta - \\theta_0)\\).\n\n\n\n\nTable 3.6:  A crude guide to understanding the relationship between statistical significance and effect sizes. Basically, if you don’t have a significant result then the effect size is pretty meaningless because you don’t have any evidence that it’s even real. On the other hand, if you do have a significant effect but your effect size is small then there’s a pretty good chance that your result (although real) isn’t all that interesting. However, this guide is very crude. It depends a lot on what exactly you’re studying. Small effects can be of massive practical importance in some situations. So don’t take this table too seriously. It’s a rough guide at best. \n\nbig effect sizesmall effect size\n\nsignificant resultdifference is real, and  of practical importancedifference is real, but   might not be interesting\n\nnon-significant resultno effect observedno effect observed\n\n\n\n\n\nWhy calculate effect size? Let’s assume that you’ve run your experiment, collected the data, and gotten a significant effect when you ran your hypothesis test. Isn’t it enough just to say that you’ve gotten a significant effect? Surely that’s the point of hypothesis testing? Well, sort of. Yes, the point of doing a hypothesis test is to try to demonstrate that the null hypothesis is wrong, but that’s hardly the only thing we’re interested in. If the null hypothesis claimed that \\(\\theta = .5\\) and we show that it’s wrong, we’ve only really told half of the story. Rejecting the null hypothesis implies that we believe that \\(\\theta \\neq .5\\), but there’s a big difference between \\(\\theta = .51\\) and \\(\\theta = .8\\). If we find that \\(\\theta = .8\\), then not only have we found that the null hypothesis is wrong, it appears to be very wrong. On the other hand, suppose we’ve successfully rejected the null hypothesis, but it looks like the true value of \\(\\theta\\) is only .51 (this would only be possible with a very large study). Sure, the null hypothesis is wrong but it’s not at all clear that we actually care because the effect size is so small. In the context of my ESP study we might still care since any demonstration of real psychic powers would actually be pretty cool10, but in other contexts a \\(1\\%\\) difference usually isn’t very interesting, even if it is a real difference. For instance, suppose we’re looking at differences in high school exam scores between males and females and it turns out that the female scores are \\(1\\%\\) higher on average than the males. If I’ve got data from thousands of students then this difference will almost certainly be statistically significant, but regardless of how small the p value is it’s just not very interesting. You’d hardly want to go around proclaiming a crisis in boys education on the basis of such a tiny difference would you? It’s for this reason that it is becoming more standard (slowly, but surely) to report some kind of standard measure of effect size along with the the results of the hypothesis test. The hypothesis test itself tells you whether you should believe that the effect you have observed is real (i.e., not just due to chance), whereas the effect size tells you whether or not you should care.\n\n\n3.8.3 Increasing the power of your study\nNot surprisingly, scientists are fairly obsessed with maximising the power of their experiments. We want our experiments to work and so we want to maximise the chance of rejecting the null hypothesis if it is false (and of course we usually want to believe that it is false!). As we’ve seen, one factor that influences power is the effect size. So the first thing you can do to increase your power is to increase the effect size. In practice, what this means is that you want to design your study in such a way that the effect size gets magnified. For instance, in my ESP study I might believe that psychic powers work best in a quiet, darkened room with fewer distractions to cloud the mind. Therefore I would try to conduct my experiments in just such an environment. If I can strengthen people’s ESP abilities somehow then the true value of \\(\\theta\\) will go up 11 and therefore my effect size will be larger. In short, clever experimental design is one way to boost power, because it can alter the effect size.\nUnfortunately, it’s often the case that even with the best of experimental designs you may have only a small effect. Perhaps, for example, ESP really does exist but even under the best of conditions it’s very very weak. Under those circumstances your best bet for increasing power is to increase the sample size. In general, the more observations that you have available, the more likely it is that you can discriminate between two hypotheses. If I ran my ESP experiment with 10 participants and 7 of them correctly guessed the colour of the hidden card you wouldn’t be terribly impressed. But if I ran it with 10,000 participants, and 7,000 of them got the answer right, you would be much more likely to think I had discovered something. In other words, power increases with the sample size. This is illustrated in Figure 3.8, which shows the power of the test for a true parameter of \\(\\theta = 0.7\\) for all sample sizes \\(N\\) from \\(1\\) to \\(100\\), where I’m assuming that the null hypothesis predicts that \\(\\theta_0 = 0.5\\).\n\n\n\n\n\nFigure 3.8: The power of our test plotted as a function of the sample size \\(N\\). In this case, the true value of \\(\\theta\\) is 0.7 but the null hypothesis is that \\(\\theta = 0.5\\). Overall, larger \\(N\\) means greater power. (The small zig-zags in this function occur because of some odd interactions between \\(\\theta\\), \\(\\alpha\\) and the fact that the binomial distribution is discrete, it doesn’t matter for any serious purpose)\n\n\n\n\nBecause power is important, whenever you’re contemplating running an experiment it would be pretty useful to know how much power you’re likely to have. It’s never possible to know for sure since you can’t possibly know what your real effect size is. However, it’s often (well, sometimes) possible to guess how big it should be. If so, you can guess what sample size you need! This idea is called power analysis, and if it’s feasible to do it then it’s very helpful. It can tell you something about whether you have enough time or money to be able to run the experiment successfully. It’s increasingly common to see people arguing that power analysis should be a required part of experimental design, so it’s worth knowing about. I don’t discuss power analysis in this book, however. This is partly for a boring reason and partly for a substantive one. The boring reason is that I haven’t had time to write about power analysis yet. The substantive one is that I’m still a little suspicious of power analysis. Speaking as a researcher, I have very rarely found myself in a position to be able to do one. It’s either the case that (a) my experiment is a bit non-standard and I don’t know how to define effect size properly, or (b) I literally have so little idea about what the effect size will be that I wouldn’t know how to interpret the answers. Not only that, after extensive conversations with someone who does stats consulting for a living (my wife, as it happens), I can’t help but notice that in practice the only time anyone ever asks her for a power analysis is when she’s helping someone write a grant application. In other words, the only time any scientist ever seems to want a power analysis in real life is when they’re being forced to do it by bureaucratic process. It’s not part of anyone’s day to day work. In short, I’ve always been of the view that whilst power is an important concept, power analysis is not as useful as people make it sound, except in the rare cases where (a) someone has figured out how to calculate power for your actual experimental design and (b) you have a pretty good idea what the effect size is likely to be.12 Maybe other people have had better experiences than me, but I’ve personally never been in a situation where both (a) and (b) were true. Maybe I’ll be convinced otherwise in the future, and probably a future version of this book would include a more detailed discussion of power analysis, but for now this is about as much as I’m comfortable saying about the topic."
  },
  {
    "objectID": "09-Hypothesis-testing.html#some-issues-to-consider",
    "href": "09-Hypothesis-testing.html#some-issues-to-consider",
    "title": "3  Hypothesis testing",
    "section": "3.9 Some issues to consider",
    "text": "3.9 Some issues to consider\nWhat I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity because it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.\n\n3.9.1 Neyman versus Fisher\nThe first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman (see Lehmann (2011) for a historical summary). The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.\nFirst, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null) and that what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the p-value. According to Fisher, if the null hypothesis provided a very poor account of the data then you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all there is to it.\nIn contrast, Neyman thought that the point of hypothesis testing was as a guide to action and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could do (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the \\(p\\) value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.\nAs you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually13 define the \\(p\\) value in terms of exreme data (Fisher), but we still have \\(\\alpha\\) values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess, but I hope this at least explains why it’s a mess.\n\n\n3.9.2 Bayesians versus frequentists\nEarlier on in this chapter I was quite emphatic about the fact that you cannot interpret the p value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter 1) and as such it does not allow you to assign probabilities to hypotheses. The null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a \\(10\\%\\) chance that the null hypothesis is true. That’s just a reflection of the degree of confidence that you have in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom”. It doesn’t have one!\nMost importantly, this isn’t a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. I’ll talk more about this in ?sec-Bayesian-statistics, but for now what I want to point out to you is the p value is a terrible approximation to the probability that \\(H_0\\) is true. If what you want to know is the probability of the null, then the p value is not what you’re looking for!\n\n\n3.9.3 Traps\nAs you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is thoughtlessness. I don’t mean stupidity, I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.\nTo give an example of this, consider the following example (see Gelman & Stern (2006)). Suppose I’m running my ESP study and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, \\(33\\) out of \\(50\\) guessed the colour of the card correctly. This is a significant effect (\\(p = .03\\)). Of the female participants, \\(29\\) out of \\(50\\) guessed correctly. This is not a significant effect (\\(p = .32\\)). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t actually run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,14 but when we do that it turns out that we have no evidence that males and females are significantly different (\\(p = .54\\)). Now do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline. By pure chance one of them happened to end up on the magic side of the \\(p = .05\\) line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it. The difference between significant and not-significant is not evidence of a real difference. If you want to say that there’s a difference between two groups, then you have to test for that difference!\nThe example above is just that, an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about what it is you want to test, why you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world."
  },
  {
    "objectID": "09-Hypothesis-testing.html#summary",
    "href": "09-Hypothesis-testing.html#summary",
    "title": "3  Hypothesis testing",
    "section": "3.10 Summary",
    "text": "3.10 Summary\nNull hypothesis testing is one of the most ubiquitous elements to statistical theory. The vast majority of scientific papers report the results of some hypothesis test or another. As a consequence it is almost impossible to get by in science without having at least a cursory understanding of what a p-value means, making this one of the most important chapters in the book. As usual, I’ll end the chapter with a quick recap of the key ideas that we’ve talked about:\n\nA menagerie of hypotheses. Research hypotheses and statistical hypotheses. Null and alternative hypotheses.\nTwo types of errors. Type I and Type II.\nTest statistics and sampling distributions.\nHypothesis testing for Making decisions\nThe p value of a test. p-values as “soft” decisions\nReporting the results of a hypothesis test\nRunning the hypothesis test in practice\nEffect size, sample size and power\nSome issues to consider regarding hypothesis testing\n\nLater in the book, in ?sec-Bayesian-statistics, I’ll revisit the theory of null hypothesis tests from a Bayesian perspective and introduce a number of new tools that you can use if you aren’t particularly fond of the orthodox approach. But for now, though, we’re done with the abstract statistical theory, and we can start discussing specific data analysis tools.\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes: Statistical power, meta-analysis, and the interpretation of research results. Cambridge University Press.\n\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60, 328–331.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation of classical statistics. Springer."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Cohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes:\nStatistical power, meta-analysis, and the interpretation of research\nresults. Cambridge University Press.\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical\ndistributions (3rd ed). Wiley.\n\n\nFisher, R. A. (1922). On the mathematical foundation of theoretical\nstatistics. Philosophical Transactions of the Royal Society A,\n222, 309–368.\n\n\nGelman, A., & Stern, H. (2006). The difference between\n“significant” and “not significant” is not\nitself statistically significant. The American Statistician,\n60, 328–331.\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan;\nCompany.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation\nof classical statistics. Springer.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A\nmethodological paradox. Philosophy of Science, 34,\n103–115.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard\nUniversity Press."
  }
]